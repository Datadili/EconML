{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35064a4e",
   "metadata": {},
   "source": [
    "# ECON 566 - Pset 3 - Dili Maduabum, Joshua Bailey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e1907",
   "metadata": {},
   "source": [
    "## PROBLEM 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0e04a",
   "metadata": {},
   "source": [
    "### PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bded31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, x0, grad_f, step_size_func, epsilon, gamma):\n",
    "    \"\"\"\n",
    "    Performs gradient descent optimisation to minimize function f.\n",
    "\n",
    "    Parameters:\n",
    "    - f: Function to minimise.\n",
    "    - x0: Starting point, array of shape (p,).\n",
    "    - grad_f: Gradient of f, function returning array.\n",
    "    - step_size_func: Function to calculate step size, takes parameters (f, grad_f_at_xk, k, xk, gamma).\n",
    "    - epsilon: Stopping criterion threshold for the gradient norm.\n",
    "    - gamma: Tuning parameter for step size calculation.\n",
    "\n",
    "    Returns:\n",
    "    - xk: The approximation to the minimum.\n",
    "    - k: Number of iterations performed.\n",
    "    \"\"\"\n",
    "    xk = x0\n",
    "    k = 0\n",
    "    while np.linalg.norm(grad_f(xk)) > epsilon:\n",
    "        sz = step_size_func(f, grad_f(xk), k, xk, gamma)\n",
    "        xk = xk - sz * grad_f(xk)\n",
    "        k += 1\n",
    "    \n",
    "    return xk, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82b14c",
   "metadata": {},
   "source": [
    "### PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f241f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant step size\n",
    "def constant_step_size(f, grad_f_at_xk, k, xk, gamma=0.01):\n",
    "    \"\"\"\n",
    "    Computes a constant step size.\n",
    "\n",
    "    Parameters:\n",
    "    - f, grad_f_at_xk, k, xk: Parameters as above (not used in this version).\n",
    "    - gamma: Constant step size.\n",
    "\n",
    "    Returns:\n",
    "    - gamma: The constant step size.\n",
    "    \"\"\"\n",
    "    return gamma\n",
    "\n",
    "# Decreasing step size \n",
    "def decreasing_step_size(f, grad_f_at_xk, k, xk, gamma=None):\n",
    "    \"\"\"\n",
    "    Computes a decreasing step size, inversely proportional to the iteration number.\n",
    "\n",
    "    Parameters:\n",
    "    - f, grad_f_at_xk, xk, gamma: Parameters as above (not used in this version).\n",
    "    - k: The current iteration number.\n",
    "\n",
    "    Returns:\n",
    "    - step size: A value decreasing with k, specifically 1/k.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 1  # Handle division by zero for the first iteration\n",
    "    else:\n",
    "        return 1 / k\n",
    "\n",
    "# Backtracking line search\n",
    "def backtracking_line_search(f, grad_f_at_xk, k, xk, gamma=0.9, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Performs backtracking line search to compute the step size.\n",
    "\n",
    "    Parameters:\n",
    "    - f: The function being minimised.\n",
    "    - grad_f_at_xk: The gradient of f at the current point xk.\n",
    "    - k: The current iteration number (unused in this function).\n",
    "    - xk: The current point in the iteration.\n",
    "    - gamma: Reduction factor for the step size, default is 0.9.\n",
    "    - alpha: Parameter controlling the sufficiency of decrease, default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - t: The computed step size after backtracking.\n",
    "    \"\"\"\n",
    "    t = 1\n",
    "    while f(xk - t * grad_f_at_xk) > f(xk) - alpha * t * np.dot(grad_f_at_xk, grad_f_at_xk):\n",
    "        t *= gamma\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7af32",
   "metadata": {},
   "source": [
    "### PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163aa8ba-2b31-49d2-96ce-2e5f7efef791",
   "metadata": {},
   "source": [
    "#### 3(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a683c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return 2*x**4 - 9*x**3 + 4*x**2 + 11*x + 3\n",
    "\n",
    "# Generate x values from -2 to 4\n",
    "x_values = np.linspace(-2, 4, 400)\n",
    "\n",
    "# Compute y values\n",
    "y_values = f(x_values)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_values, y_values, label='f(x) = 2x^4 - 9x^3 + 4x^2 + 11x + 3')\n",
    "plt.title('Plot of f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae1713-ac79-43ee-9912-dc807708af91",
   "metadata": {},
   "source": [
    "#### 3(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974c59d-9d31-479b-9fdf-a297a11958ed",
   "metadata": {},
   "source": [
    "No, the function is not convex in the interval $[-2, 4]$. It exhibits multiple distinct pairs of points on the graph of the function where the line segment connecting the points is not above the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c6f28-d18d-4643-acc6-781deae127f2",
   "metadata": {},
   "source": [
    "#### 3(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef77252c-f15f-41d9-81c4-6ded455b233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant: Iterations = 21, Time = 0.0007 seconds, x_min = [-0.48136221]\n",
      "Decreasing: Iterations = 34, Time = 0.0010 seconds, x_min = [2.85638891]\n",
      "Backtracking: Iterations = 84, Time = 0.0592 seconds, x_min = [2.85639764]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Function\n",
    "def f(x):\n",
    "    return 2*x**4 - 9*x**3 + 4*x**2 + 11*x + 3\n",
    "\n",
    "# Gradient of the function\n",
    "def grad_f(x):\n",
    "    return 8*x**3 - 27*x**2 + 8*x + 11\n",
    "\n",
    "# Gradient descent algorithm\n",
    "def gradient_descent(f, x0, grad_f, step_size_func, epsilon, gamma=0.01, lower_bound=-2, upper_bound=4):\n",
    "    xk = x0\n",
    "    k = 0\n",
    "    while np.linalg.norm(grad_f(xk)) > epsilon:\n",
    "        sz = step_size_func(f, grad_f(xk), k, xk, gamma)\n",
    "        xk_new = xk - sz * grad_f(xk)\n",
    "        xk_new = np.clip(xk_new, lower_bound, upper_bound) # Optimise over the interval\n",
    "        xk = xk_new\n",
    "        k += 1\n",
    "    return xk, k\n",
    "\n",
    "#Implement the three methods\n",
    "# Constant step size\n",
    "def constant_step_size(f, grad_f_at_xk, k, xk, gamma=0.01):\n",
    "    return gamma\n",
    "\n",
    "# Decreasing step size\n",
    "def decreasing_step_size(f, grad_f_at_xk, k, xk, gamma=None):\n",
    "    return 1 / (k + 1)  # +1 to handle the case when k=0\n",
    "\n",
    "# Backtracking line search\n",
    "def backtracking_line_search(f, grad_f_at_xk, k, xk, gamma=0.9, alpha=0.01):\n",
    "    t = 1\n",
    "    while f(xk - t * grad_f_at_xk) > f(xk) - alpha * t * np.dot(grad_f_at_xk, grad_f_at_xk):\n",
    "        t *= gamma\n",
    "    return t\n",
    "\n",
    "# Parameters\n",
    "x0 = np.array([-2])\n",
    "epsilon = 0.001\n",
    "\n",
    "# Results\n",
    "results = {}\n",
    "\n",
    "# Applying gradient descent with each method\n",
    "for method_name, step_size_func, gamma_val in [\n",
    "    (\"Constant\", constant_step_size, 0.01),\n",
    "    (\"Decreasing\", decreasing_step_size, None),\n",
    "    (\"Backtracking\", backtracking_line_search, 0.9)]:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    x_min, iterations = gradient_descent(f, x0, grad_f, step_size_func, epsilon, gamma=gamma_val)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    results[method_name] = {\n",
    "        \"Iterations\": iterations, \n",
    "        \"Time\": end_time - start_time, \n",
    "        \"x_min\": x_min\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for method, info in results.items():\n",
    "    print(f\"{method}: Iterations = {info['Iterations']}, Time = {info['Time']:.4f} seconds, x_min = {info['x_min']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672697c-4ce6-46e9-ae06-123a3c7865a5",
   "metadata": {},
   "source": [
    "The decreasing and backtracking methods found the global minimum but the decreasing method was quicker (34 vs. 84 iterations, and 0.0018 vs 0.0611 seconds, respectively). The constant method converged to the other local minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd3962-a42a-4825-8e82-290582a67ec2",
   "metadata": {},
   "source": [
    "#### 3(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd25ca2d-0500-4427-9503-23ec49d21589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant: Iterations = 0, Time = 0.0000 seconds, x_min = [1]\n",
      "Decreasing: Iterations = 0, Time = 0.0000 seconds, x_min = [1]\n",
      "Backtracking: Iterations = 0, Time = 0.0000 seconds, x_min = [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Function\n",
    "def f(x):\n",
    "    return 2*x**4 - 9*x**3 + 4*x**2 + 11*x + 3\n",
    "\n",
    "# Gradient of the function\n",
    "def grad_f(x):\n",
    "    return 8*x**3 - 27*x**2 + 8*x + 11\n",
    "\n",
    "# Gradient descent algorithm\n",
    "def gradient_descent(f, x0, grad_f, step_size_func, epsilon, gamma=0.01, lower_bound=-2, upper_bound=4):\n",
    "    xk = x0\n",
    "    k = 0\n",
    "    while np.linalg.norm(grad_f(xk)) > epsilon:\n",
    "        sz = step_size_func(f, grad_f(xk), k, xk, gamma)\n",
    "        xk_new = xk - sz * grad_f(xk)\n",
    "        xk_new = np.clip(xk_new, lower_bound, upper_bound) # Optimise over the interval\n",
    "        xk = xk_new\n",
    "        k += 1\n",
    "    return xk, k\n",
    "\n",
    "#Implement the three methods\n",
    "# Constant step size\n",
    "def constant_step_size(f, grad_f_at_xk, k, xk, gamma=0.01):\n",
    "    return gamma\n",
    "\n",
    "# Decreasing step size\n",
    "def decreasing_step_size(f, grad_f_at_xk, k, xk, gamma=None):\n",
    "    return 1 / (k + 1)  # +1 to handle the case when k=0\n",
    "\n",
    "# Backtracking line search\n",
    "def backtracking_line_search(f, grad_f_at_xk, k, xk, gamma=0.9, alpha=0.01):\n",
    "    t = 1\n",
    "    while f(xk - t * grad_f_at_xk) > f(xk) - alpha * t * np.dot(grad_f_at_xk, grad_f_at_xk):\n",
    "        t *= gamma\n",
    "    return t\n",
    "\n",
    "# Parameters\n",
    "x0 = np.array([1])\n",
    "epsilon = 0.001\n",
    "\n",
    "# Results\n",
    "results = {}\n",
    "\n",
    "# Applying gradient descent with each method\n",
    "for method_name, step_size_func, gamma_val in [\n",
    "    (\"Constant\", constant_step_size, 0.01),\n",
    "    (\"Decreasing\", decreasing_step_size, None),\n",
    "    (\"Backtracking\", backtracking_line_search, 0.9)]:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    x_min, iterations = gradient_descent(f, x0, grad_f, step_size_func, epsilon, gamma=gamma_val)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    results[method_name] = {\n",
    "        \"Iterations\": iterations, \n",
    "        \"Time\": end_time - start_time, \n",
    "        \"x_min\": x_min\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for method, info in results.items():\n",
    "    print(f\"{method}: Iterations = {info['Iterations']}, Time = {info['Time']:.4f} seconds, x_min = {info['x_min']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b968239-339c-473e-97f4-ff95f57d4bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "All methods converged to the local maximum the algorithm began on (discussed below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a29b3-9ccf-404e-84f4-a61b851a2978",
   "metadata": {},
   "source": [
    "#### 3(e)\n",
    "Yes, in part(d) the algorithm got stuck at $x=1$, the gradient ($\\nabla f(x)$) is zero, which can lead the algorithm to conclude it has found an optimum since the stopping criterion is based on the gradient's magnitude ($|\\nabla f(x^k)\\|_2 \\leq \\varepsilon$). Specifically, the algorithm checks if the norm of the gradient is less than or equal to a small epsilon value. When this condition fails, the iterations stop. The algorithm may \"think\" it has already converged to an optimal solution and terminate immediately, performing no iterations.\n",
    "\n",
    "This limitation of shows the importance of the starting point selection. It also could be addressed by modifying the algorithm, for instance, adding small perturbations if the algorithm starts at a point with a zero gradient.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142a714",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "### PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb115a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "g_{x, y}(\\beta) = y - \\sum^p_{j =1} x_i\\beta_j, \\text{ $\\forall \\lambda \\in [0,1]$, $\\beta^1, \\beta^2 \\in \\mathbb{R}^p$}\n",
    "\\end{equation}\n",
    "$$\n",
    "is convex if:\n",
    "\n",
    "$g_{x, y}(\\lambda\\beta^1 + (1 - \\lambda)\\beta^2) \\le \\lambda g_{x, y}(\\beta^1) + (1-\\lambda)g_{x, y}(\\beta^2)$ \n",
    "\n",
    "Proof:\n",
    "\n",
    "$g_{x, y}(\\lambda\\beta^1 + (1 - \\lambda)\\beta^2)$ \n",
    "\n",
    "$= y - \\sum^p_{j =1} x_i(\\lambda\\beta^1 + (1 - \\lambda)\\beta^2)_j$\n",
    "\n",
    "$= y - \\lambda\\sum^p_{j =1} x_i\\beta_j^1 - (1 - \\lambda)\\sum^p_{j =1}x_i\\beta_j^2$\n",
    "\n",
    "$= y - \\lambda\\sum^p_{j =1} x_i\\beta_j^1 - \\sum^p_{j =1}x_i\\beta_j^2 + \\lambda \\sum^p_{j =1}x_i\\beta_j^2$\n",
    "\n",
    "$\\pm \\lambda y$\n",
    "\n",
    "$= y + \\pmb{\\lambda y} - \\lambda \\sum^p_{j =1}x_i\\beta_j^1 - \\sum^p_{j =1}x_i\\beta_j^2 - \\pmb{\\lambda y} + \\lambda \\sum^p_{j =1}x_i\\beta_j^2$\n",
    "\n",
    "Collect like terms..\n",
    "\n",
    "$= \\lambda (y - \\sum^p_{j =1}x_i\\beta_j^1) + (1-\\lambda)y - (1-\\lambda)\\sum^p_{j =1}x_i\\beta_j^2$\n",
    "\n",
    "$= \\lambda (y - \\sum^p_{j =1}x_i\\beta_j^1) + (1-\\lambda)(y - \\sum^p_{j =1}x_i\\beta_j^2)$\n",
    "\n",
    "$= \\lambda g_{x, y}(\\beta^1) + (1-\\lambda)g_{x, y}(\\beta^2)$\n",
    "\n",
    "which is convex by definition, since they are equal, by linearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af830472",
   "metadata": {},
   "source": [
    "$$\\require{cancel}$$\n",
    "### PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c6a0d",
   "metadata": {},
   "source": [
    "$h(z) = z^2$ is convex if:\n",
    "\n",
    "$h(\\lambda x + (1 - \\lambda)y) \\le \\lambda h(x) + (1-\\lambda)h(y)$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$h(\\lambda x + (1 - \\lambda)y)$\n",
    "\n",
    "$= (\\lambda x + (1 - \\lambda)y)^2$\n",
    "\n",
    "$= (\\lambda x)^2 + ((1 - \\lambda)y)^2 + 2(\\lambda x)((1 - \\lambda)y)$\n",
    "\n",
    "$= (\\lambda x)^2 + ((1 - \\lambda)y)^2 + (2xy)\\lambda(1 - \\lambda)$\n",
    "\n",
    "$\\le \\lambda^2 x^2 + (1 - \\lambda)^2y^2 + (x^2+ y^2)\\lambda(1-\\lambda)$, by youngs inequality\n",
    "\n",
    "$\\le \\lambda^2 x^2 + (1 - \\lambda)^2y^2 + x^2\\lambda(1-\\lambda) + y^2\\lambda(1-\\lambda)$\n",
    "\n",
    "$\\le (\\lambda^2 + \\lambda(1-\\lambda))x^2 + ((1 - \\lambda)^2 + \\lambda(1-\\lambda))y^2$\n",
    "\n",
    "$\\le (\\cancel{\\lambda^2}+\\lambda \\cancel{-\\lambda^2})x^2 + (1 - 2\\lambda \\cancel{+ \\lambda^2} + \\lambda \\cancel{-\\lambda^2})y^2$\n",
    "\n",
    "$\\le \\lambda x^2 + (1-\\lambda)y^2$\n",
    "\n",
    "$\\le \\lambda h(x) + (1-\\lambda)h(y)$///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f14e",
   "metadata": {},
   "source": [
    "### PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c3510",
   "metadata": {},
   "source": [
    "$q_{x,y}(\\beta) = h(g_{x,y}(\\beta))$ is convex if\n",
    "\n",
    "$h(\\lambda g_{x,y}(a) + (1-\\lambda)g_{x,y}(b)) \\le \\lambda h(g_{x,y}(a)) + (1-\\lambda)h(g_{x,y}(b)) \\text{     $\\forall a,b \\in \\mathbb{R}^p$}$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$h(\\lambda g_{x,y}(a) + (1-\\lambda)g_{x,y}(b))$\n",
    "\n",
    "$= (\\lambda g_{x,y}(a) + (1-\\lambda)g_{x,y}(b))^2$\n",
    "\n",
    "$= (\\lambda (y - \\sum^p_{j =1}x_ia_j) + (1-\\lambda)(y - \\sum^p_{j =1}x_ib_j))^2$\n",
    "\n",
    "$= (\\cancel{\\lambda y} - \\lambda \\sum^p_{j =1}x_ia_j + y - \\sum^p_{j =1}x_ib_j \\cancel{-\\lambda y} + \\lambda \\sum^p_{j =1}x_ib_j))^2$\n",
    "\n",
    "$= (- \\lambda \\sum^p_{j =1}x_ia_j + y - \\sum^p_{j =1}x_ib_j  + \\lambda \\sum^p_{j =1}x_ib_j))^2$\n",
    "\n",
    "$= (( y - \\lambda \\sum^p_{j =1}x_ia_j)- (\\sum^p_{j =1}x_ib_j - \\lambda \\sum^p_{j =1}x_ib_j))^2$\n",
    "\n",
    "$= (y - \\lambda \\sum^p_{j =1}x_ia_j)^2 -2(y - \\lambda \\sum^p_{j =1}x_ia_j)(\\sum^p_{j =1}x_ib_j - \\lambda \\sum^p_{j =1}x_ib_j)$\n",
    "$+ (\\sum^p_{j =1}x_ib_j - \\lambda \\sum^p_{j =1}x_ib_j)^2$\n",
    "\n",
    "\n",
    "$= (y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2) - 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j$ \n",
    "$+ \\lambda 2(\\sum^p_{j =1}x_ia_j\\sum^p_{j =1}x_ib_j) - \\lambda^2 2(\\sum^p_{j =1}x_ia_j\\sum^p_{j =1}x_ib_j)+ ((\\sum^p_{j =1}x_ib_j)^2$ \n",
    "$- \\lambda2(\\sum^p_{j =1}x_ib_j)^2 + \\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)$\n",
    "\n",
    "$= (y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2)- 2y\\sum^p_{j =1}x_ib_j$\n",
    "$+ \\lambda 2y\\sum^p_{j =1}x_ib_j + (\\lambda  - \\lambda^2)[2(\\sum^p_{j =1}x_ia_j\\sum^p_{j =1}x_ib_j)]$\n",
    "$+ ((\\sum^p_{j =1}x_ib_j)^2 - \\lambda2(\\sum^p_{j =1}x_ib_j)^2+\\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)$\n",
    "\n",
    "$\\le (y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2)$ \n",
    "$- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j + (\\lambda  - \\lambda^2)\\pmb{[(\\sum^p_{j =1}x_ia_j)^2 + (\\sum^p_{j =1}x_ib_j)^2]} $\n",
    "$+ ( (\\sum^p_{j =1}x_ib_j)^2 - \\lambda2(\\sum^p_{j =1}x_ib_j)^2 + \\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)$, by Young's Inequality\n",
    "\n",
    "$\\le (y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2) - 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j + \\lambda(\\sum^p_{j =1}x_ia_j)^2 $\n",
    "$+ \\lambda(\\sum^p_{j =1}x_ib_j)^2 - \\lambda^2(\\sum^p_{j =1}x_ia_j)^2 - \\lambda^2(\\sum^p_{j =1}x_ib_j)^2 + ( (\\sum^p_{j =1}x_ib_j)^2 - \\lambda2(\\sum^p_{j =1}x_ib_j)^2 $\n",
    "$+ \\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)$\n",
    "\n",
    "$\\le y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j +  \\cancel{\\lambda^2 (\\sum^p_{j =1}x_ia_j)^2}- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j $\n",
    "$+ \\lambda(\\sum^p_{j =1}x_ia_j)^2 + \\lambda(\\sum^p_{j =1}x_ib_j)^2  \\cancel{- \\lambda^2(\\sum^p_{j =1}x_ia_j)^2}  \\cancel{- \\lambda^2(\\sum^p_{j =1}x_ib_j)^2} $\n",
    "$+  (\\sum^p_{j =1}x_ib_j)^2 - \\lambda2(\\sum^p_{j =1}x_ib_j)^2 +  \\cancel{\\lambda^2 (\\sum^p_{j =1}x_ib_j)^2}$\n",
    "\n",
    "$\\le y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j$ \n",
    "$+ \\lambda(\\sum^p_{j =1}x_ia_j)^2 - \\lambda(\\sum^p_{j =1}x_ib_j)^2  +  (\\sum^p_{j =1}x_ib_j)^2$\n",
    "\n",
    "Collect like terms:\n",
    "\n",
    "$\\le  -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda(\\sum^p_{j =1}x_ia_j)^2+ y^2 - 2y\\sum^p_{j =1}x_ib_j +  (\\sum^p_{j =1}x_ib_j)^2 $\n",
    "$+ \\lambda 2y\\sum^p_{j =1}x_ib_j- \\lambda(\\sum^p_{j =1}x_ib_j)^2 $\n",
    "\n",
    "\n",
    "Add and substract $\\lambda y^2$:\n",
    "\n",
    "\n",
    "$ \\le \\pmb{\\lambda y^2} -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda(\\sum^p_{j =1}x_ia_j)^2+ y^2 - 2y\\sum^p_{j =1}x_ib_j +  (\\sum^p_{j =1}x_ib_j)^2 $\n",
    "$-\\pmb{\\lambda y^2}+ \\lambda 2y\\sum^p_{j =1}x_ib_j - \\lambda(\\sum^p_{j =1}x_ib_j)^2 $\n",
    "\n",
    "\n",
    "$ \\le \\lambda(y^2 - 2y\\sum^p_{j =1}x_ia_j + (\\sum^p_{j =1}x_ia_j)^2) + (1 - \\lambda)(y^2 - 2y\\sum^p_{j =1}x_ib_j + (\\sum^p_{j =1}x_ib_j)^2) $\n",
    "\n",
    "\n",
    "$ \\le \\lambda(y - \\sum^p_{j =1}x_ia_j)^2 + (1 - \\lambda)(y - \\sum^p_{j =1}x_ib_j)^2 $\n",
    "\n",
    "$ \\le \\lambda(g_{x,y}(a))^2 + (1 - \\lambda)(g_{x,y}(b))^2 $\n",
    "\n",
    "$ \\le \\lambda h(g_{x,y}(a)) + (1 - \\lambda)h(g_{x,y}(b))$ ///\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f7bf2",
   "metadata": {},
   "source": [
    "### PART 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60841bdd",
   "metadata": {},
   "source": [
    "$t(\\lambda x + (1 - \\lambda)y)$ \n",
    "\n",
    "$= r(\\lambda x + (1 - \\lambda)y) + s(\\lambda x + (1 - \\lambda)y)$\n",
    "\n",
    "$\\le \\lambda r(x) + (1 - \\lambda)r(y) + \\lambda s(x) + (1 - \\lambda)s(y)$ since $r(z)$ and $s(z)$ are convex\n",
    "\n",
    "$\\le \\lambda (r(x) +s(x)) + (1 - \\lambda)(r(y) + s(y))$\n",
    "\n",
    "$\\le \\lambda t(x) + (1 - \\lambda) t(y)$\n",
    "\n",
    "which is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826db7c",
   "metadata": {},
   "source": [
    "### PART 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d940ad",
   "metadata": {},
   "source": [
    "Claim:\n",
    "$$\\begin{equation}\n",
    "v(z) = \\sum^{n}_{i=1}v_i(z)\n",
    "\\end{equation}$$\n",
    "\n",
    "is convex.\n",
    "\n",
    "Proof by induction:\n",
    "\n",
    "<u>Base Case</u>\n",
    "\n",
    "From part 4 above, we saw that:\n",
    "\n",
    "$r(\\lambda x + (1 - \\lambda)y) + s(\\lambda x + (1 - \\lambda)y) \\le \\lambda (r(x) +s(x)) + (1 - \\lambda)(r(y) + s(y))$\n",
    "\n",
    "let $r(z) = v_1(z)$ and $s(z) = v_2(z)$. Thus, for n = 2,\n",
    "\n",
    "$v_1(\\lambda x + (1 - \\lambda)y) + v_2(\\lambda x + (1 - \\lambda)y) \\le \\lambda (v_1(x) +v_2(x)) + (1 - \\lambda)(v_1(y) + v_2(y))$\n",
    "\n",
    "<u>Inductive Hypothesis</u>\n",
    "\n",
    "$\\sum^m_{i=1}v_i(\\lambda x + (1 - \\lambda)y) \\le \\lambda \\sum^m_{i=1}v_i(x) + (1 - \\lambda)\\sum^m_{i=1}v_i(y)$ for some finite m\n",
    "\n",
    "<u>Inductive Step</u>\n",
    "\n",
    "Consider:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\sum^{m+1}_{i=1}v_i(\\lambda x + (1 - \\lambda)y)\n",
    "\\end{equation}$$\n",
    "\n",
    "$= \\sum^m_{i=1}v_i(\\lambda x + (1 - \\lambda)y) + v_{m+1}(\\lambda x + (1-\\lambda)y)$\n",
    "\n",
    "$\\le \\underbrace{\\lambda \\sum^m_{i=1}v_i(x) + (1 - \\lambda)\\sum^m_{i=1}v_i(y)}_{\\text{by the inductive hypothesis}} + \\underbrace{\\lambda v_{m+1}(x)+ (1-\\lambda)v_{m+1}(y)}_{\\text{since $v_i(z)$ is convex}}$\n",
    "\n",
    "collect like terms:\n",
    "\n",
    "$\\le \\lambda (\\sum^m_{i=1}v_i(x) + v_{m+1}(x)) + (1 - \\lambda)(\\sum^m_{i=1}v_i(y) + v_{m+1}(y))$\n",
    "\n",
    "$\\le \\lambda \\sum^{m+1}_{i=1}v_i(x) + (1 - \\lambda)\\sum^{m+1}_{i=1}v_i(y)$\n",
    "\n",
    "Thus by induction, $v(z) = \\sum^{n}_{i=1}v_i(z)$ is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d5a94",
   "metadata": {},
   "source": [
    "### PART 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bbe11",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "f(\\beta) = \\sum^n_{i = 1} (y_i - \\sum^p_{j =1} x_i\\beta_j)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "is convex if:\n",
    "\n",
    "$f(\\lambda a + (1-\\lambda)b) \\le \\lambda f(a) + (1-\\lambda)f(b)$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$f(\\lambda a + (1-\\lambda)b)$\n",
    "\n",
    "\n",
    "$= \\sum^n_{i = 1} [y_i - \\sum^p_{j =1}x_i(\\lambda a + (1-\\lambda)b)_j]^2$\n",
    "\n",
    "$= \\sum^n_{i = 1} (y_i - \\lambda \\sum^p_{j =1}x_ia_j - \\sum^p_{j =1}x_ib_j +\\lambda \\sum^p_{j =1}x_ib_j)^2$\n",
    "\n",
    "$= \\sum^n_{i = 1} (( y_i - \\lambda \\sum^p_{j =1}x_ia_j)- (\\sum^p_{j =1}x_ib_j - \\lambda \\sum^p_{j =1}x_ib_j))^2$\n",
    "\n",
    "\n",
    "$=\\sum^n_{i =1} [(y_i - \\lambda \\sum^p_{j =1}x_ia_j)^2 -2(y - \\lambda \\sum^p_{j =1}x_ia_j)(\\sum^p_{j =1}x_ib_j - \\lambda \\sum^p_{j =1}x_ib_j)$\n",
    "$+ (\\sum^p_{j =1}x_ib_j - \\lambda \\sum^p_{j =1}x_ib_j)^2]$\n",
    "\n",
    "\n",
    "$=\\sum^n_{i =1} [(y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2) - 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j$ \n",
    "$+ \\lambda 2(\\sum^p_{j =1}x_ia_j\\sum^p_{j =1}x_ib_j) - \\lambda^2 2(\\sum^p_{j =1}x_ia_j\\sum^p_{j =1}x_ib_j)+ ((\\sum^p_{j =1}x_ib_j)^2$ \n",
    "$- \\lambda2(\\sum^p_{j =1}x_ib_j)^2 + \\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)]$\n",
    "\n",
    "$=\\sum^n_{i =1} [(y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2)- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j$\n",
    "$+ (\\lambda  - \\lambda^2)[2(\\sum^p_{j =1}x_ia_j\\sum^p_{j =1}x_ib_j)]$\n",
    "$+ ((\\sum^p_{j =1}x_ib_j)^2 - \\lambda 2(\\sum^p_{j =1}x_ib_j)^2+\\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)]$\n",
    "\n",
    "$\\le \\sum^n_{i =1} [(y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2)$ \n",
    "$- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j + (\\lambda  - \\lambda^2)\\pmb{[(\\sum^p_{j =1}x_ia_j)^2 + (\\sum^p_{j =1}x_ib_j)^2]} $\n",
    "$+ ( (\\sum^p_{j =1}x_ib_j)^2 - \\lambda 2(\\sum^p_{j =1}x_ib_j)^2 + \\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)]$, by Young's Inequality\n",
    "\n",
    "$\\le \\sum^n_{i =1}[(y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda^2 (\\sum^p_{j =1}x_ia_j)^2) - 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j$\n",
    "$+ \\lambda(\\sum^p_{j =1}x_ia_j)^2 + \\lambda(\\sum^p_{j =1}x_ib_j)^2 - \\lambda^2(\\sum^p_{j =1}x_ia_j)^2 - \\lambda^2(\\sum^p_{j =1}x_ib_j)^2$\n",
    "$+ ( (\\sum^p_{j =1}x_ib_j)^2 - \\lambda2(\\sum^p_{j =1}x_ib_j)^2 + \\lambda^2 (\\sum^p_{j =1}x_ib_j)^2)]$\n",
    "\n",
    "$\\le \\sum^n_{i =1}[y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j +  \\cancel{\\lambda^2 (\\sum^p_{j =1}x_ia_j)^2}- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j $\n",
    "$+ \\lambda(\\sum^p_{j =1}x_ia_j)^2 + \\lambda(\\sum^p_{j =1}x_ib_j)^2  \\cancel{- \\lambda^2(\\sum^p_{j =1}x_ia_j)^2}  \\cancel{- \\lambda^2(\\sum^p_{j =1}x_ib_j)^2} $\n",
    "$+  (\\sum^p_{j =1}x_ib_j)^2 - \\lambda2(\\sum^p_{j =1}x_ib_j)^2 +  \\cancel{\\lambda^2 (\\sum^p_{j =1}x_ib_j)^2}]$\n",
    "\n",
    "$\\le \\sum^n_{i =1} [y^2 -\\lambda 2y\\sum^p_{j =1}x_ia_j- 2y\\sum^p_{j =1}x_ib_j + \\lambda 2y\\sum^p_{j =1}x_ib_j$ \n",
    "$+ \\lambda(\\sum^p_{j =1}x_ia_j)^2 - \\lambda(\\sum^p_{j =1}x_ib_j)^2  +  (\\sum^p_{j =1}x_ib_j)^2]$\n",
    "\n",
    "Collect like terms:\n",
    "\n",
    "$\\le \\sum^n_{i =1} [ -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda(\\sum^p_{j =1}x_ia_j)^2+ y^2 - 2y\\sum^p_{j =1}x_ib_j +  (\\sum^p_{j =1}x_ib_j)^2 $\n",
    "$+ \\lambda 2y\\sum^p_{j =1}x_ib_j- \\lambda(\\sum^p_{j =1}x_ib_j)^2 ]$\n",
    "\n",
    "\n",
    "Add and substract $\\lambda y^2$:\n",
    "\n",
    "$ \\le \\sum^n_{i =1} (\\pmb{\\lambda y^2} -\\lambda 2y\\sum^p_{j =1}x_ia_j + \\lambda(\\sum^p_{j =1}x_ia_j)^2)$\n",
    "$+ \\sum^n_{i =1} (y^2 - 2y\\sum^p_{j =1}x_ib_j +  (\\sum^p_{j =1}x_ib_j)^2)$ \n",
    "$+ \\sum^n_{i =1} (-\\pmb{\\lambda y^2}+ \\lambda 2y\\sum^p_{j =1}x_ib_j - \\lambda(\\sum^p_{j =1}x_ib_j)^2) $\n",
    "\n",
    "$ \\le \\sum^n_{i =1} \\lambda(y^2 - 2y\\sum^p_{j =1}x_ia_j + (\\sum^p_{j =1}x_ia_j)^2) $\n",
    "$+\\sum^n_{i =1}(y^2 - 2y\\sum^p_{j =1}x_ib_j + (\\sum^p_{j =1}x_ib_j)^2)$\n",
    "$- \\lambda\\sum^n_{i =1}(y^2 - 2y\\sum^p_{j =1}x_ib_j + (\\sum^p_{j =1}x_ib_j)^2)$\n",
    "\n",
    "\n",
    "\n",
    "$ \\le \\sum^n_{i =1} \\lambda(y^2 - 2y\\sum^p_{j =1}x_ia_j + (\\sum^p_{j =1}x_ia_j)^2)$ \n",
    "$+ (1 - \\lambda)\\sum^n_{i =1}(y^2 - 2y\\sum^p_{j =1}x_ib_j + (\\sum^p_{j =1}x_ib_j)^2) $\n",
    "\n",
    "$ \\le \\lambda\\sum^n_{i =1}(y - \\sum^p_{j =1}x_ia_j)^2 + (1 - \\lambda)\\sum^n_{i =1}(y - \\sum^p_{j =1}x_ib_j)^2 $\n",
    "\n",
    "$ \\le \\lambda f(a) + (1 - \\lambda)f(b) $///\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2de106",
   "metadata": {},
   "source": [
    "### PART 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869f941",
   "metadata": {},
   "source": [
    "The function isn't convex, it could converge to a global minima instead of a global maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4047bb",
   "metadata": {},
   "source": [
    "## PROBLEM 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb9665",
   "metadata": {},
   "source": [
    "### PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbddee",
   "metadata": {},
   "source": [
    "Machine learning relies heavily on optimization as it makes it possible to train precise models, enhances generalizationperformance, and makes machine learning algorithms more scalable so they can efficiently solve real-world issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6ed9a",
   "metadata": {},
   "source": [
    "### PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ba69b",
   "metadata": {},
   "source": [
    "The optimal solution to a regression problem is to find the value that minimizes the mean squared error. In general, we would prefer the dataset with the the true function $((X_i, f(X_i)))_{i = 1,...,n}$, because it would not have the additional variability introduced by the error term $U$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e6cac",
   "metadata": {},
   "source": [
    "### PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d37df",
   "metadata": {},
   "source": [
    "if $f(x)$ is convex, then:\n",
    "\n",
    "$f(\\lambda x + (1 - \\lambda)y) = \\lambda f(x) + (1 - \\lambda)f(y)$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$f(\\lambda x + (1 - \\lambda)y)$\n",
    "\n",
    "$\\le f(\\lambda x) + f((1 - \\lambda)y)$, by the $f(x + y) \\le f(x) + f(y)$ property  \n",
    "\n",
    "$\\le \\lambda f(x) + (1 - \\lambda)f(y)$, by the $f(ax) = af(x)$ property\n",
    "\n",
    "///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14451dd0",
   "metadata": {},
   "source": [
    "### PART 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b95bb-6842-4bbc-9c4c-d56a2460e2f3",
   "metadata": {},
   "source": [
    "The relationship between maximisation and minimisation problems can be intricately understood through the concepts of primal and dual problems. \n",
    "\n",
    "The primal problem is the original optimisation problem. The primal problem seeks to maximise an objective function subject to a set of constraints.\n",
    "\n",
    "The dual problem is derived from the primal problem but takes a different form. For a primal maximisation problem, the dual will be a minimisation problem. The dual problem involves maximising or minimising a different objective function under a different set of constraints, which are related to the original (primal) constraints.\n",
    "\n",
    "The transformation from a primal maximisation to a dual minimisation problem (and vice versa) is based on the principle of duality. The duality principle states that every optimisation problem (the primal problem) inherently contains within it another problem (the dual problem), where the dual of the dual problem is the primal problem itself.\n",
    "\n",
    "For example, a convex primal problem where the goal is to maximise a concave utility function subject to inequality constraints:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{maximize} \\quad & f(x) \\\\\n",
    "\\text{subject to} \\quad & g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{align*}\n",
    "$$\n",
    "where:\n",
    "- $ f(x)$ is a concave function that we want to maximise.\n",
    "- $ g_i(x) $ are convex functions representing the constraints, ensuring that the feasible region is convex.\n",
    "\n",
    "Since $f(x) $ is concave, its negative $ -f(x) $ is convex.\n",
    "\n",
    "The Lagrangian $ L(x, \\lambda) $ for the primal problem combines the objective function and the constraints using Lagrange multipliers $ \\lambda $:\n",
    "\n",
    "$$\n",
    "L(x, \\lambda) = f(x) - \\sum_{i=1}^{m} \\lambda_i g_i(x)\n",
    "$$\n",
    "where:\n",
    "- $ \\lambda_i \\geq 0 $ are the Lagrange multipliers for the inequality constraints.\n",
    "\n",
    "The dual function $ g(\\lambda) $ is defined as the infimum of the Lagrangian over $ x $:\n",
    "\n",
    "$$\n",
    "g(\\lambda) = \\inf_x L(x, \\lambda) = \\inf_x \\left( f(x) - \\sum_{i=1}^{m} \\lambda_i g_i(x) \\right)\n",
    "$$\n",
    "\n",
    "The dual problem seeks to minimise the negative of the dual function with respect to $ \\lambda $, subject to $ \\lambda $ being non-negative:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad & -g(\\lambda) \\\\\n",
    "\\text{subject to} \\quad & \\lambda_i \\geq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the primal maximisation problem, we aim to find the maximum value of $ f(x) $ within the feasible region defined by the constraints $ g_i(x) $. In the dual minimisation problem, we are looking for the minimum value of $ -g(\\lambda) $, which indirectly gives us the tightest lower bound on the maximum value of $ f(x) $ from the primal problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb90498",
   "metadata": {},
   "source": [
    "## PROBLEM 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc7409",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc3dc9-f44b-42de-b763-f4a06059a658",
   "metadata": {},
   "source": [
    "In convex optimization, the objective is to minimise a convex function over a set defined by convex constraints. A fundamental property of convex optimisation problems is the convexity of the feasible set, which is the set of all points satisfying the problem's constraints. The convexity of the feasible set ensures that any linear combination of feasible points is also feasible, a property that influences the solvability of convex problems.\n",
    "\n",
    "Let's consider a convex optimisation problem where the objective is to minimise a convex function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, subject to $m$ constraints $g_i(x) \\leq 0$ for $i = 1, \\ldots, m$, where each $g_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a convex function. The feasible set $S$ is defined as $S = \\{x \\in \\mathbb{R}^n \\mid g_i(x) \\leq 0, \\, i = 1, \\ldots, m\\}$.\n",
    "\n",
    "To prove that $S$ is convex, we take any two points $x_1, x_2 \\in S$ and a scalar $\\theta$ such that $0 \\leq \\theta \\leq 1$. We need to show that the point $x_{\\theta} = (1 - \\theta)x_1 + \\theta x_2$ also belongs to $S$.\n",
    "\n",
    "Given the convexity of each $g_i(x)$, for $i = 1, \\ldots, m$, we apply the definition of convex functions to get:\n",
    "\n",
    "$g_i((1 - \\theta)x_1 + \\theta x_2) \\leq (1 - \\theta)g_i(x_1) + \\theta g_i(x_2)$\n",
    "\n",
    "Since $x_1, x_2 \\in S$, it follows that $g_i(x_1) \\leq 0$ and $g_i(x_2) \\leq 0$ for all $i$. Therefore, the right-hand side of the inequality is a convex combination of numbers less than or equal to zero, which implies:\n",
    "\n",
    "$(1 - \\theta)g_i(x_1) + \\theta g_i(x_2) \\leq 0$\n",
    "\n",
    "Hence, $g_i(x_{\\theta}) \\leq 0$ for all $i$, indicating that $x_{\\theta}$ satisfies all the constraints and therefore $x_{\\theta} \\in S$.\n",
    "\n",
    "Since $x_{\\theta}$ is an arbitrary linear combination of $x_1$ and $x_2$ with $0 \\leq \\theta \\leq 1$ and $x_{\\theta} \\in S$, we conclude that $S$ is convex. This shows that the feasible set in a convex optimisation problem is convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b41316",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f829c3-680f-478f-bb21-e430a301d198",
   "metadata": {},
   "source": [
    "To show that any local optimum in a convex optimisation problem is a global optimum, we proceed with a proof by contradiction, leveraging the convexity of both the objective function and the feasible set.\n",
    "\n",
    "As above, a convex optimisation problem seeks to minimize a convex function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ subject to constraints that define a convex feasible set $S$. We assume $x^*$ is a local optimum but not the global optimum.\n",
    "\n",
    "It therefore follows that, by contradiction:\n",
    "\n",
    "1. Suppose there exists $x' \\in S$ such that $f(x') < f(x^*)$, implying $x^*$ is not the global optimum.\n",
    "\n",
    "2. As in Part 1, the feasible set $S$ is convex. Therefore, for any $0 \\leq \\theta \\leq 1$, the point $x_\\theta = \\theta x' + (1 - \\theta)x^*$ is also in $S$.\n",
    "\n",
    "3. The convexity of $f$ ensures that for any $0 \\leq \\theta \\leq 1$:\n",
    "   $$\n",
    "   f(x_\\theta) \\leq \\theta f(x') + (1 - \\theta)f(x^*)\n",
    "   $$\n",
    "\n",
    "4. By contradiction, choosing $0 < \\theta < 1$ yields:\n",
    "   $$\n",
    "   f(x_\\theta) < \\theta f(x^*) + (1 - \\theta)f(x^*) = f(x^*)\n",
    "   $$\n",
    "   This indicates $f(x_\\theta) < f(x^*)$, contradicting the assumption that $x^*$ is a local optimum since $x_\\theta$, being within the local neighborhood of $x^*$, should not have a lower function value.\n",
    "\n",
    "It therefore follows that the assumption that $x^*$ is not a global optimum leads to a contradiction. Therefore, any local optimum in a convex optimisation problem must be a global optimum, confirming the fundamental property of convex optimisation that local optimality implies global optimality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd773f-1bd4-44ad-a071-49072d1050dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
