{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c099ab6",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053fa185",
   "metadata": {},
   "source": [
    "###Â Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbba677",
   "metadata": {},
   "source": [
    "Consider a typical neural network setup, where each layer's output, $ h^l $, is computed using a weight matrix $ W^l $, a bias $ b^l $, and an activation function $ f^l $. \n",
    "\n",
    "Given that the output of layer $ l $ is defined by:\n",
    "$$ h^l = f^l(W^l h^{l-1} + b^l) $$\n",
    "where $ f^l $ is the activation function applied element-wise.\n",
    "\n",
    "To show that:\n",
    "$$ \\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "we can use the chain rule:\n",
    "\n",
    "1. Start by noting the function composition in $ h^l $:\n",
    "   $$ h^l = f^l(z^l) $$\n",
    "   where $ z^l = W^l h^{l-1} + b^l $.\n",
    "\n",
    "2. The derivative of $ h^l $ with respect to $ b^l $ is:\n",
    "   $$ \\frac{\\partial h^l}{\\partial b^l} = \\frac{\\partial f^l}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial b^l} $$\n",
    "   \n",
    "   Applying the chain rule, we have:\n",
    "$$\\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}\\left(W^l h^{l-1} + b^l\\right) \\cdot \\frac{\\partial (W^l h^{l-1} + b^l)}{\\partial b^l}$$\n",
    "\n",
    "3. Since $ z^l $ is linear in $ b^l $, the derivative $ \\frac{\\partial z^l}{\\partial b^l} $ is straightforwardly 1 (keeping in mind that $ b^l $ is added element-wise to $ W^l h^{l-1} $).\n",
    "\n",
    "4. Therefore, $ \\frac{\\partial h^l}{\\partial b^l} $ simplifies to:\n",
    "   $$ \\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}(z^l) $$\n",
    "   which is exactly:\n",
    "   $$ \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c8458",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563691d",
   "metadata": {},
   "source": [
    "Compute the gradient of the loss function $ L $ with respect to the biases $ b^l $ using the chain rule in the context of backpropagation. \n",
    "\n",
    "1. Recognize that the gradient of the loss $ L $ with respect to the biases $ b^l $ can be propagated from the output backwards using the chain rule:\n",
    "   $$ \\frac{\\partial L}{\\partial b^l} = \\frac{\\partial L}{\\partial h^l} \\cdot \\frac{\\partial h^l}{\\partial b^l} $$\n",
    "\n",
    "2. From Part 1, we know:\n",
    "   $$ \\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "\n",
    "3. The derivative $ \\frac{\\partial L}{\\partial h^l} $ is calculated during backpropagation as the \"upstream gradient\" from later layers, often denoted as $ \\delta^l $. It represents how the change in $ h^l $ impacts the change in loss.\n",
    "\n",
    "4. Combining these, the formula for $ \\frac{\\partial L}{\\partial b^l} $ in terms of the gradients from later stages in the network is:\n",
    "   $$ \\frac{\\partial L}{\\partial b^l} = \\delta^l \\odot \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "   where $ \\odot $ denotes the element-wise product.\n",
    "\n",
    "By backpropagating the gradient from the output layer back to the inputs using these steps, we ensure that the biases $ b^l $ are updated in a way that minimizes the loss $ L $, thereby improving the model's performance with each training iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c36a3",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eb67b",
   "metadata": {},
   "source": [
    "_Back ground functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "41e08b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the activation function\n",
    "def act_func(x, type):\n",
    "    \"\"\"\n",
    "    Compute the activation function for a given type.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - type (str): Type of activation function ('sigmoid' or 'ReLU').\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Output of the activation function.\n",
    "    \"\"\"\n",
    "    if type == \"sigmoid\":\n",
    "        # Compute the sigmoid activation function: 1 / (1 + exp(-x))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    if type == \"ReLU\":\n",
    "        # Compute the ReLU activation function: max(0, x)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    \n",
    "#get H's [sigma(WX + b)] and the Z's [WX + b]\n",
    "def feed_forward(X, nl, act:list, parameters: dict):\n",
    "    \"\"\"\n",
    "    Perform the feedforward pass through the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): Input data.\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - act (list): List of activation functions for each layer.\n",
    "    - parameters (dict): Dictionary containing the parameters of the neural network.\n",
    "\n",
    "    Returns:\n",
    "    - forward: Dictionary containing the forward pass computations (ZL and HL)\n",
    "    \"\"\"\n",
    "    p = parameters\n",
    "    forward = {}\n",
    "    forward[\"H0\"] = X  # Input layer is the initial value of H0\n",
    "    L = nl \n",
    "    for l in range(1, L + 1):\n",
    "        # Calculate the linear transformation Zl = Wl * Hl-1 + Bl\n",
    "        forward[\"Z\" + str(l)] = np.dot(p[\"W\" + str(l)], forward[\"H\" + str(l - 1)]) \\\n",
    "                               + p[\"B\" + str(l)]\n",
    "        # Apply the activation function to compute Hl\n",
    "        forward[\"H\" + str(l)] = act_func(forward[\"Z\" + str(l)], act[l-1])\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbf9c8",
   "metadata": {},
   "source": [
    "### Part 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4f45d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_pred, lambd: float, parameters: list):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) loss function \n",
    "    with L2 penalty (Ridge Regression).\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The true target values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "    - lambd (float): The regularization parameter for the L2 penalty.\n",
    "    - parameters (dict): A dictionary containing the parameters of the neural network.\n",
    "\n",
    "    Returns:\n",
    "    - MSE (float): The MSE loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    # Calculate L2 penalty (sum of squares of all parameters)\n",
    "    # For each layer, square the parameters and then sum their sums\n",
    "    # parameters.values() returns the value for each key in the dict\n",
    "    L2_penalty = np.sum([np.sum(param**2) for param in parameters.values()])\n",
    "    # Compute MSE with L2 penalty\n",
    "    MSE = (1/2) * np.linalg.norm(y - y_pred)**2 + lambd * L2_penalty\n",
    "    return MSE\n",
    "\n",
    "def Cross_entropy(y, y_pred, lambd: float, parameters: list):\n",
    "    \"\"\"\n",
    "    TO DO: Compute the cross entropy loss\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15518c4d",
   "metadata": {},
   "source": [
    "### Part 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50b850",
   "metadata": {},
   "source": [
    "__To do__... math derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b37ced59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad_wrt_y_pred(y,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the Mean Squared Error (MSE) loss function \n",
    "    with respect to the predicted value (last output of neural network)\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The true target values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of the MSE loss with respect to the predicted values.\n",
    "    \"\"\"\n",
    "    mse_grad = (y_pred - y)\n",
    "    return mse_grad\n",
    "\n",
    "\n",
    "def Cross_entrpy_grad_wrt_y_pred(y,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the cross entropy loss with L2 penalty \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf2a6d",
   "metadata": {},
   "source": [
    "### Part 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8b883e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_derivative(type, Zl):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the activation function with respect to its input.\n",
    "    \n",
    "    Parameters:\n",
    "    - activation (str): Type of activation function ('sigmoid' or 'ReLU').\n",
    "    - Zl (numpy.ndarray): Input array to the activation function (pre-activation values).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Derivative of the activation function evaluated at Zl.\n",
    "    \"\"\"\n",
    "    if type == \"sigmoid\":\n",
    "        # Compute the sigmoid of Zl\n",
    "        sigmoid = 1 / (1 + np.exp(-Zl))\n",
    "        # Compute the derivative of the sigmoid function\n",
    "        derivative = sigmoid * (1 - sigmoid)\n",
    "        return derivative\n",
    "    \n",
    "    if type == \"ReLU\":\n",
    "        # Compute derivative: 1 when input > 0, 0 otherwise\n",
    "        derivative = np.where(Zl > 0, 1, 0)\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308347f4",
   "metadata": {},
   "source": [
    "### Part 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e1a4a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine updated with doc string and comments\n",
    "def backward_propagation(nl, nh, Y, lambd, parameters, forward, act:list):\n",
    "    \"\"\"\n",
    "    Perform backward propagation for a neural network to compute gradients \n",
    "    for all parameters.\n",
    "\n",
    "    Args:\n",
    "    nl (int): Number of layers in the neural network.\n",
    "    nh (list): an int vec of length nl - that has the number of neurons in each layer\n",
    "    Y (array): Observed values we try to predict\n",
    "    lambd (float): Regularization parameter.\n",
    "    parameters (dict): Dict containing parameters 'W' (weights) and 'B' (biases)\n",
    "    forward (dict): Dictionary containing the forward pass computations (ZL and HL).\n",
    "    act (list):a str vec of length nl - the activation function used in each layer \n",
    "\n",
    "    Returns:\n",
    "    g (dict): A dictionary containing gradients of loss with respect to each parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = parameters\n",
    "    f = forward\n",
    "    L = nl\n",
    "    g = {}\n",
    "    \n",
    "    HL = f[\"H\"+str(L)] #Final prediction sigma(W^{L}H^{L-1} + B^{L})\n",
    "    ZL = f[\"Z\"+str(L)] #Last layer's Z (W^{L}H^{L-1} + B^{L})\n",
    "    \n",
    "    # Compute gradient of loss wrt last layer Z (dL_dHL*dHL_dZL)\n",
    "    g[\"dLoss_dZ\"+str(L)] = MSE_grad_wrt_y_pred(Y, HL) * act_derivative(act[-1], ZL) \n",
    "    \n",
    "    # Deriative of last layer Z wrt its weights & biases (dZL_dWL, dZL_dBL)\n",
    "    g[\"dZ\"+str(L)+\"_dW\"+str(L)] = f[\"H\"+str(L-1)] #just H^{L-1}\n",
    "    g[\"dZ\"+str(L)+\"_dB\"+str(L)] = np.ones((nh[-1], 1))#vec of 1's row's of last layer neurons\n",
    "    \n",
    "    # Calculate derivative with respect to weights and biases for the last layer\n",
    "    # dLoss_dWL = dL_dHL*dHL_dZL*dZL_dWL, dLoss_dBL = dL_dHL*dHL_dZL*dZL_dBL\n",
    "    g[\"dLoss_dW\"+str(L)] = np.dot(g[\"dLoss_dZ\"+str(L)], \n",
    "                                 g[\"dZ\"+str(L)+\"_dW\"+str(L)].T) \\\n",
    "                           + lambd * p[\"W\"+str(L)]  # Include regularization term\n",
    "    g[\"dLoss_dB\"+str(L)] = np.dot(g[\"dLoss_dZ\"+str(L)].T,\n",
    "                                 g[\"dZ\"+str(L)+\"_dB\"+str(L)]) \\\n",
    "                           + lambd * p[\"B\"+str(L)]  # Include regularization term\n",
    "    \n",
    "    #from L-1 to first layer, which is 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        # Calculate gradient of Z in layer l+1 wrt Z in layer l\n",
    "        # dZl+1_dZl = dZl+1_dHl*dHl_dZl \n",
    "        g[\"dZ_\"+str(l+1)+\"_dZ\"+str(l)] = np.dot(p[\"W\"+str(l+1)].T, \n",
    "                                                g[\"dLoss_dZ\"+str(l+1)])\n",
    "        \n",
    "        # Propagate the loss gradient back from layer l+1 to layer l\n",
    "        #dLoss_dZl = dLoss_dZl+1*dZl+1_dHl*dHl_dZl\n",
    "        g[\"dLoss_dZ\"+str(l)] = g[\"dZ_\"+str(l+1)+\"_dZ\"+str(l)] * \\\n",
    "                                act_derivative(act[l], f[\"Z\"+str(l)])\n",
    "        \n",
    "        # Deriative of Z wrt its weights & biases (for each layer)\n",
    "        g[\"dZ\"+str(l)+\"_dW\"+str(l)] = f[\"H\" + str(l-1)]\n",
    "        g[\"dZ\"+str(l)+\"_dB\"+str(l)] = np.ones((nh[l], 1))\n",
    "\n",
    "        # Calculate derivatives with respect to weights and biases in layer l\n",
    "        g[\"dLoss_dW\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l)], \n",
    "                                     g[\"dZ\"+str(l)+ \"_dW\"+str(l)].T) \\\n",
    "                               + lambd * p[\"W\" + str(l)]\n",
    "        g[\"dLoss_dB\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l)].T,\n",
    "                                      g[\"dZ\"+str(l)+ \"_dB\"+str(l)]) \\\n",
    "                                + lambd * p[\"B\" + str(l)]\n",
    "                                \n",
    "        \n",
    "                              \n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3f023",
   "metadata": {},
   "source": [
    "### Part 3b\n",
    "\n",
    "_loading the neccesities from pset 8_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c4caea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 5\n",
    "X = np.array([0.1, -0.2, 0.3, -0.4, 0.5]).reshape(-1, 1)\n",
    "nh = [X.shape[0], 5, 4, 3, 5, 1]\n",
    "act = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"] \n",
    "Y = 3\n",
    "\n",
    "def parameter(nl):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    \n",
    "    Returns:\n",
    "    - parameters (dict): Dictionary containing all the weights and biases\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for l in range(1, nl + 1): #1 to 5\n",
    "        # Select the values all but the last one (which is the bias)\n",
    "        parameters[\"W\" + str(l)] = pd.read_csv(f'data/layer{l}.csv').values[:, :-1]\n",
    "        # Select the values the last one only (the bias), and make it a column vector\n",
    "        parameters[\"B\" + str(l)] = pd.read_csv(f'data/layer{l}.csv').values[:, -1].reshape(-1,1)\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b0b69",
   "metadata": {},
   "source": [
    "_computing the back prop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dc216761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (MSE) of the weights in respect to Weights of the first hidden layer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.062636</td>\n",
       "      <td>-0.082066</td>\n",
       "      <td>0.151207</td>\n",
       "      <td>-0.004532</td>\n",
       "      <td>0.091946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.048763</td>\n",
       "      <td>0.038955</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>0.078164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.083558</td>\n",
       "      <td>0.073823</td>\n",
       "      <td>-0.062109</td>\n",
       "      <td>0.094364</td>\n",
       "      <td>0.007481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159528</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>-0.221471</td>\n",
       "      <td>0.082124</td>\n",
       "      <td>-0.198937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032952</td>\n",
       "      <td>-0.030541</td>\n",
       "      <td>0.112497</td>\n",
       "      <td>0.059385</td>\n",
       "      <td>0.061989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0 -0.062636 -0.082066  0.151207 -0.004532  0.091946\n",
       "1  0.018354  0.048763  0.038955 -0.001580  0.078164\n",
       "2 -0.083558  0.073823 -0.062109  0.094364  0.007481\n",
       "3  0.159528  0.057579 -0.221471  0.082124 -0.198937\n",
       "4  0.032952 -0.030541  0.112497  0.059385  0.061989"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (MSE)of the bias in respect to Bias of the first hidden layer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.147018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.047758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.005556\n",
       "1 -0.015523\n",
       "2 -0.147018\n",
       "3 -0.047758\n",
       "4  0.041851"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (Cross Entropy) of the weights in respect to Weights of the first hidden layer:\n",
      "Gradient (Cross Entropy) of the weights in respect to Bias of the first hidden layer:\n"
     ]
    }
   ],
   "source": [
    "parameters = parameter(nl)\n",
    "forward = feed_forward(X, nl, act, parameters)\n",
    "lambd = 1 #random since now was given\n",
    "\n",
    "grads = backward_propagation(nl, nh, Y, lambd, parameters, forward, act)\n",
    "\n",
    "print(\"Gradient (MSE) of the weights in respect to Weights of the first hidden layer:\")\n",
    "display(pd.DataFrame(grads['dLoss_dW1']))\n",
    "print(\"Gradient (MSE)of the bias in respect to Bias of the first hidden layer:\")\n",
    "display(pd.DataFrame(grads['dLoss_dB1']))\n",
    "\n",
    "print(\"Gradient (Cross Entropy) of the weights in respect to Weights of the first hidden layer:\")\n",
    "#enter display here\n",
    "print(\"Gradient (Cross Entropy) of the weights in respect to Bias of the first hidden layer:\")\n",
    "#enter display here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b946db2",
   "metadata": {},
   "source": [
    "*Appendix, Finding the dimensions of each*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "101970e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dLoss_dZ5: (1, 1)\n",
      "shape of dLoss_dW5: (1, 5)\n",
      "shape of dLoss_dB5: (1, 1) \n",
      "\n",
      "shape of dLoss_dZ4: (5, 1)\n",
      "shape of dLoss_dW4: (5, 3)\n",
      "shape of dLoss_dB4: (5, 1) \n",
      "\n",
      "shape of dLoss_dZ3: (3, 1)\n",
      "shape of dLoss_dW3: (3, 4)\n",
      "shape of dLoss_dB3: (3, 1) \n",
      "\n",
      "shape of dLoss_dZ2: (4, 1)\n",
      "shape of dLoss_dW2: (4, 5)\n",
      "shape of dLoss_dB2: (4, 1) \n",
      "\n",
      "shape of dLoss_dZ1: (5, 1)\n",
      "shape of dLoss_dW1: (5, 5)\n",
      "shape of dLoss_dB1: (5, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(1, nl  + 1)):\n",
    "    print(\"shape of dLoss_dZ\" +str(i) + \":\", grads[\"dLoss_dZ\"+str(i)].shape)\n",
    "    print(\"shape of dLoss_dW\" +str(i) + \":\", grads[\"dLoss_dW\"+str(i)].shape)\n",
    "    print(\"shape of dLoss_dB\" +str(i) + \":\", grads[\"dLoss_dB\"+str(i)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdb8f4",
   "metadata": {},
   "source": [
    "### Part 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0584cf",
   "metadata": {},
   "source": [
    "blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a65c8",
   "metadata": {},
   "source": [
    "### Part 4b\n",
    "\n",
    "_New parameter funcction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "42df8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter(nl, nh, X):\n",
    "    nh.insert(0, X.shape[0]) # add the row of the X's to the first one\n",
    "    nh[-1] = Y.shape[0] #changing the \"1\" to be the layer of neur\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) of a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - nh (list): List containing the number of neurons in each layer.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing the initialized (random parameters).\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for n in range(1, nl + 1):\n",
    "        # Initialize weights randomly from a uniform distribution\n",
    "        parameters[\"W\" + str(n)] = np.random.rand(nh[n], nh[n-1])\n",
    "        # Initialize biases as zeros\n",
    "        parameters[\"B\" + str(n)] = np.zeros((nh[n], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc25644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def stochatic_grad_descent(nh,nl, X,Y,intial_params,feed_forward,backward_propagation,batch_size,p,lambd,act, e = 0.1): \n",
    " \n",
    "    #p is the patience parameter\n",
    "\n",
    "\n",
    "    # First, randomly split data into training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    #inds = np.random.choice(range(n), size=batch_size)\n",
    "    L = nl\n",
    "    parameter = parameters(nh,nl, X_train)\n",
    "    forward = feed_forward(X_train, nl, act, parameter)\n",
    "    grads = backward_propagation(nl, nh, Y, lambd, parameter, forward, act)\n",
    "    \n",
    "    k = 0\n",
    "    grad_norm = np.linalg.norm(MSE_grad_wrt_y_pred(Y, forward[\"H\" +str(L)]))\n",
    "    inds = np.random.choice(range(n), size=batch_size)\n",
    "    \n",
    "    for l in range(1, len(parameters)//2 + 1)\n",
    "        while  np.linalg.norm(grad_norm) > e # e is epsilon\n",
    "        # w = w -n*dl_dw\n",
    "            parameter[\"W\" + str(l)] = parameter[\"W\" + str(l)] - step_size*grads[\"dLoss_dW\"+ str(l)]\n",
    "            parameter[\"B\" + str(l)] = parameter[\"B\" + str(l)] - step_size*grads[\"dLoss_dB\"+ str(l)]\n",
    "            \n",
    "            forward = feed_forward(X_train, nl, act, parameters)\n",
    "            grad_norm = np.linalg.norm(MSE_grad_wrt_y_pred(Y, forward[\"H\" +str(L)]))\n",
    "            k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7c8ee159",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,5000) and (32,5) not aligned: 5000 (dim 1) != 32 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[243], line 85\u001b[0m\n\u001b[1;32m     81\u001b[0m             epochs_no_improve \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: best_params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_val_error\u001b[39m\u001b[38;5;124m'\u001b[39m: best_val_error}\n\u001b[0;32m---> 85\u001b[0m \u001b[43mstochastic_grad_descent_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[243], line 62\u001b[0m, in \u001b[0;36mstochastic_grad_descent_with_early_stopping\u001b[0;34m(X, Y, parameters, feed_forward, backward_propagation, act, batch_size, patience, lambd, learning_rate)\u001b[0m\n\u001b[1;32m     59\u001b[0m batch_X, batch_Y \u001b[38;5;241m=\u001b[39m X_train[batch_indices], y_train[batch_indices]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Perform a forward pass\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     64\u001b[0m grads \u001b[38;5;241m=\u001b[39m backward_propagation(\u001b[38;5;28mlen\u001b[39m(act), np\u001b[38;5;241m.\u001b[39marray([nh]), batch_Y, lambd, parameters, forward, act)\n",
      "Cell \u001b[0;32mIn[146], line 41\u001b[0m, in \u001b[0;36mfeed_forward\u001b[0;34m(X, nl, act, parameters)\u001b[0m\n\u001b[1;32m     38\u001b[0m L \u001b[38;5;241m=\u001b[39m nl \n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, L \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Calculate the linear transformation Zl = Wl * Hl-1 + Bl\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     42\u001b[0m                            \u001b[38;5;241m+\u001b[39m p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)]\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Apply the activation function to compute Hl\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] \u001b[38;5;241m=\u001b[39m act_func(forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)], act[l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,5000) and (32,5) not aligned: 5000 (dim 1) != 32 (dim 0)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/ps9.csv')\n",
    "Y = data.iloc[:, 0].values.reshape(-1, 1)  # Assuming the first column is the target\n",
    "X = data.iloc[:, 1:].values  # Remaining columns are features\n",
    "\n",
    "# Define neural network structure\n",
    "nl = 5  # Number of layers\n",
    "nh = [10, 20, 15, 10, 5]  # Example: Number of neurons in each layer\n",
    "act = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"]  # Activation functions\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = parameter(nl, nh, X)\n",
    "\n",
    "# Set training parameters\n",
    "batch_size = 32\n",
    "patience = 10\n",
    "lambd = 0.01  # Regularization strength\n",
    "learning_rate = 0.01  # Learning rate for SGD updates\n",
    "\n",
    "\n",
    "\n",
    "def stochastic_grad_descent_with_early_stopping(X, Y, parameters, feed_forward, backward_propagation, act, batch_size, patience, lambd, learning_rate):\n",
    "    \"\"\"\n",
    "    Implements stochastic gradient descent with early stopping.\n",
    "    \n",
    "    Args:\n",
    "    X (numpy.ndarray): Input feature matrix.\n",
    "    Y (numpy.ndarray): Output target vector.\n",
    "    parameters (dict): Initial neural network parameters.\n",
    "    feed_forward (function): Function to perform the forward pass.\n",
    "    backward_propagation (function): Function to perform the backward pass.\n",
    "    act (list): List of activation functions for each layer.\n",
    "    batch_size (int): Size of each batch for gradient descent updates.\n",
    "    patience (int): Number of epochs to continue without improvement before stopping.\n",
    "    lambd (float): Regularization strength.\n",
    "    learning_rate (float): Learning rate for parameter updates.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the best parameters and the corresponding validation error.\n",
    "    \"\"\"\n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.333, random_state=42)\n",
    "\n",
    "    best_params = parameters.copy()\n",
    "    best_val_error = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    n_train = len(y_train)\n",
    "    indices = np.arange(n_train)\n",
    "\n",
    "    while epochs_no_improve < patience:\n",
    "        np.random.shuffle(indices)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = min(start + batch_size, n_train)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X, batch_Y = X_train[batch_indices], y_train[batch_indices]\n",
    "            \n",
    "            # Perform a forward pass\n",
    "            forward = feed_forward(batch_X, len(act), act, parameters)\n",
    "            # Compute gradients\n",
    "            grads = backward_propagation(len(act), np.array([nh]), batch_Y, lambd, parameters, forward, act)\n",
    "            \n",
    "            # Update parameters\n",
    "            for l in range(1, len(parameters)//2 + 1):\n",
    "                parameters[\"W\" + str(l)] -= learning_rate * grads[\"dLoss_dW\" + str(l)]\n",
    "                parameters[\"B\" + str(l)] -= learning_rate * grads[\"dLoss_dB\" + str(l)]\n",
    "\n",
    "        # After each epoch, evaluate on the validation set\n",
    "        val_forward = feed_forward(X_val, len(act), act, parameters)\n",
    "        val_error = MSE(y_val, val_forward[\"H\" + str(len(act))], lambd, parameters)\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_error < best_val_error:\n",
    "            best_val_error = val_error\n",
    "            best_params = {k: v.copy() for k, v in parameters.items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "    return {'best_params': best_params, 'best_val_error': best_val_error}\n",
    "\n",
    "stochastic_grad_descent_with_early_stopping(X, Y, parameters, feed_forward, backward_propagation, act, batch_size, patience, lambd, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877437c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af12321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea749f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bf63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22570f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a16f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64efba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra, helper cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4716df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter(nl, nh):\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) of a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - nh (list): List containing the number of neurons in each layer.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing the initialized parameters.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for n in range(1, nl + 1):\n",
    "        # Initialize weights randomly from a uniform distribution\n",
    "        parameters[\"W\" + str(n)] = np.random.rand(nh[n], nh[n-1])\n",
    "        # Initialize biases as zeros\n",
    "        parameters[\"B\" + str(n)] = np.zeros((nh[n], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3f8f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 5\n",
    "X = np.array([0.1, -0.2, 0.3, -0.4, 0.5]).reshape(-1, 1)\n",
    "nh = [X.shape[0], 5, 4, 3, 5, 1]\n",
    "act = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"] \n",
    "Y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246fa192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = np.array([1, 2, 3, 0, 1 , -1, -2 , -34])\n",
    "np.where(sigma < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "725470f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.35"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = np.array([1, 1, 3, 4])\n",
    "\n",
    "#np.sum(parameters**2)\n",
    "\n",
    "#np.square(parameters)\n",
    "#0.05*np.sum(parameters**2)\n",
    "0.05*np.sum(np.square(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cd08721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\n",
    "    np.array([[1, 1, 3, 4],  # First matrix\n",
    "              [1, 2, 3, 4]]),\n",
    "    np.array([[2, 1, 3, 4],  # Second matrix\n",
    "              [1, 5, 3, 4]]),\n",
    "    np.array([1, 2])         # Vector\n",
    "]\n",
    "\n",
    "np.sum([np.sum(param) for param in parameters])\n",
    "#for param in parameters:\n",
    "#    print(np.sum(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c28896ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dili/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,4) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,4) into shape (2,)"
     ]
    }
   ],
   "source": [
    "np.sum(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b79c4280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 1)\n",
      "(4, 5)\n",
      "(4, 1)\n",
      "(3, 4)\n",
      "(3, 1)\n",
      "(5, 3)\n",
      "(5, 1)\n",
      "(1, 5)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "for parm in parameters.values():\n",
    "    print(parm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13f36e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "B1\n",
      "W2\n",
      "B2\n",
      "W3\n",
      "B3\n",
      "W4\n",
      "B4\n",
      "W5\n",
      "B5\n"
     ]
    }
   ],
   "source": [
    "for parm in parameters:\n",
    "    print(parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4186d55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39526413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(nh[-2]).reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7132b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in reversed(range(1, 5)):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "65827b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6de4f89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((nh[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b82054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((nh[-2], 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d02020f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H0': array([[ 0.1],\n",
       "        [-0.2],\n",
       "        [ 0.3],\n",
       "        [-0.4],\n",
       "        [ 0.5]]),\n",
       " 'Z1': array([[ 0.0976316 ],\n",
       "        [ 0.02795802],\n",
       "        [-0.22286044],\n",
       "        [-0.24213525],\n",
       "        [ 0.09218016]]),\n",
       " 'H1': array([[0.0976316 ],\n",
       "        [0.02795802],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.09218016]]),\n",
       " 'Z2': array([[ 0.05258633],\n",
       "        [-0.05628207],\n",
       "        [ 0.0357589 ],\n",
       "        [-0.10550538]]),\n",
       " 'H2': array([[0.51314355],\n",
       "        [0.4859332 ],\n",
       "        [0.50893877],\n",
       "        [0.47364809]]),\n",
       " 'Z3': array([[-0.03414519],\n",
       "        [ 0.23866093],\n",
       "        [ 0.03396447]]),\n",
       " 'H3': array([[0.        ],\n",
       "        [0.23866093],\n",
       "        [0.03396447]]),\n",
       " 'Z4': array([[ 0.02747301],\n",
       "        [ 0.03831927],\n",
       "        [ 0.09527121],\n",
       "        [-0.0263907 ],\n",
       "        [ 0.0336014 ]]),\n",
       " 'H4': array([[0.50686782],\n",
       "        [0.50957864],\n",
       "        [0.5237998 ],\n",
       "        [0.49340271],\n",
       "        [0.50839956]]),\n",
       " 'Z5': array([[0.30069756]]),\n",
       " 'H5': array([[0.30069756]])}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b1abd291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boo', 5, 4, 3, 5, 1]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh = [5, 4, 3, 5, 1]\n",
    "nh.insert(0, \"boo\")\n",
    "nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "936d08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dLoss_dZ5': array([[-2.69930244]]), 'dZ5_dW5': array([[0.50686782],\n",
      "       [0.50957864],\n",
      "       [0.5237998 ],\n",
      "       [0.49340271],\n",
      "       [0.50839956]]), 'dZ5_dB5': array([[1.]]), 'dLoss_W5': array([[-1.36685405, -1.37821948, -1.40785475, -1.32604112, -1.3688231 ]]), 'dLoss_B5': array([[-2.69136827]]), 'dZ_5_dZ4': array([[-0.07209804],\n",
      "       [ 0.14644256],\n",
      "       [-0.32604005],\n",
      "       [-0.31322776],\n",
      "       [-0.18900884]]), 'dLoss_dZ4': array([[-0.07209804],\n",
      "       [ 0.14644256],\n",
      "       [-0.32604005],\n",
      "       [-0.        ],\n",
      "       [-0.18900884]]), 'dZ4_dW4': array([[0.        ],\n",
      "       [0.23866093],\n",
      "       [0.03396447]]), 'dZ4_dB4': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW4': array([[ 1.08630584e-02, -2.34751526e-02, -5.39637656e-03],\n",
      "       [ 2.37754764e-03,  3.64073495e-02,  2.13050052e-03],\n",
      "       [-3.54973215e-03, -8.00294807e-02, -1.17496709e-02],\n",
      "       [ 3.05363177e-03,  5.52675816e-06,  5.89043498e-03],\n",
      "       [-4.67048816e-03, -4.47373189e-02, -1.40374193e-02]]), 'dLoss_dB4': array([[-0.43773464],\n",
      "       [-0.43903962],\n",
      "       [-0.43538887],\n",
      "       [-0.44222529],\n",
      "       [-0.43885428]]), 'dZ_4_dZ3': array([[0.03210175],\n",
      "       [0.02635444],\n",
      "       [0.02912672]]), 'dLoss_dZ3': array([[0.0080231 ],\n",
      "       [0.00649567],\n",
      "       [0.00727958]]), 'dZ3_dW3': array([[0.51314355],\n",
      "       [0.4859332 ],\n",
      "       [0.50893877],\n",
      "       [0.47364809]]), 'dZ3_dB3': array([[1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW3': array([[ 0.01128212, -0.00132198,  0.01609135,  0.00394014],\n",
      "       [ 0.01323521,  0.00600506,  0.0031097 , -0.0006397 ],\n",
      "       [ 0.00189936,  0.00286212,  0.00715356,  0.00439192]]), 'dLoss_dB3': array([[0.01277356],\n",
      "       [0.02912613],\n",
      "       [0.02256462]]), 'dZ_3_dZ2': array([[ 0.00216881],\n",
      "       [-0.00056596],\n",
      "       [ 0.00240345],\n",
      "       [-0.00032291]]), 'dLoss_dZ2': array([[ 0.00216881],\n",
      "       [-0.        ],\n",
      "       [ 0.00240345],\n",
      "       [-0.        ]]), 'dZ2_dW2': array([[0.0976316 ],\n",
      "       [0.02795802],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.09218016]]), 'dZ2_dB2': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW2': array([[ 0.00700514, -0.00682466,  0.00550013,  0.00348482,  0.00202283],\n",
      "       [-0.00051394, -0.00207497,  0.00381588,  0.00278332,  0.00384266],\n",
      "       [ 0.00217301, -0.00190425, -0.00082262, -0.00344378, -0.00034018],\n",
      "       [-0.00026903, -0.00029657, -0.00126681, -0.00353748,  0.00440554]]), 'dLoss_dB2': array([[ 0.00656279],\n",
      "       [ 0.00151213],\n",
      "       [ 0.00627786],\n",
      "       [-0.00107455]]), 'dZ_2_dZ1': array([[ 3.87846874e-04],\n",
      "       [-3.93423836e-04],\n",
      "       [ 1.99032127e-04],\n",
      "       [-1.43811296e-05],\n",
      "       [ 5.20690108e-05]]), 'dLoss_dZ1': array([[ 9.67310271e-05],\n",
      "       [-9.83367416e-05],\n",
      "       [ 4.91452800e-05],\n",
      "       [-3.54309572e-06],\n",
      "       [ 1.29896393e-05]]), 'dZ1_dW1': array([[ 0.1],\n",
      "       [-0.2],\n",
      "       [ 0.3],\n",
      "       [-0.4],\n",
      "       [ 0.5]]), 'dZ1_dB1': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW1': array([[-3.12259595e-03, -4.12168813e-03,  7.58792515e-03,\n",
      "        -2.63360456e-04,  4.64325237e-03],\n",
      "       [ 9.08382947e-04,  2.45681261e-03,  1.91971516e-03,\n",
      "        -4.16166189e-05,  3.86151313e-03],\n",
      "       [-4.17322853e-03,  3.68179447e-03, -3.09145932e-03,\n",
      "         4.69952294e-03,  3.97397557e-04],\n",
      "       [ 7.97604970e-03,  2.87961538e-03, -1.10745624e-02,\n",
      "         4.10752321e-03, -9.94853003e-03],\n",
      "       [ 1.64883782e-03, -1.52953986e-03,  5.62855148e-03,\n",
      "         2.96431075e-03,  3.10562356e-03]]), 'dLoss_dB1': array([[-0.00022366],\n",
      "       [-0.00072199],\n",
      "       [-0.00729678],\n",
      "       [-0.00233376],\n",
      "       [ 0.00214669]])}\n"
     ]
    }
   ],
   "source": [
    "print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "17079e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dLoss_dZ5': array([[-2.69930244]]), 'dZ5_dW5': array([[0.50686782],\n",
      "       [0.50957864],\n",
      "       [0.5237998 ],\n",
      "       [0.49340271],\n",
      "       [0.50839956]]), 'dZ5_dB5': array([[1.]]), 'dLoss_W5': array([[-1.36685405, -1.37821948, -1.40785475, -1.32604112, -1.3688231 ]]), 'dLoss_B5': array([[-2.69136827]]), 'dZ_5_dZ4': array([[-0.07209804],\n",
      "       [ 0.14644256],\n",
      "       [-0.32604005],\n",
      "       [-0.31322776],\n",
      "       [-0.18900884]]), 'dLoss_dZ4': array([[-0.07209804],\n",
      "       [ 0.14644256],\n",
      "       [-0.32604005],\n",
      "       [-0.        ],\n",
      "       [-0.18900884]]), 'dZ4_dW4': array([[0.        ],\n",
      "       [0.23866093],\n",
      "       [0.03396447]]), 'dZ4_dB4': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW4': array([[ 1.08630584e-02, -2.34751526e-02, -5.39637656e-03],\n",
      "       [ 2.37754764e-03,  3.64073495e-02,  2.13050052e-03],\n",
      "       [-3.54973215e-03, -8.00294807e-02, -1.17496709e-02],\n",
      "       [ 3.05363177e-03,  5.52675816e-06,  5.89043498e-03],\n",
      "       [-4.67048816e-03, -4.47373189e-02, -1.40374193e-02]]), 'dLoss_dB4': array([[-0.43773464],\n",
      "       [-0.43903962],\n",
      "       [-0.43538887],\n",
      "       [-0.44222529],\n",
      "       [-0.43885428]]), 'dZ_4_dZ3': array([[0.03210175],\n",
      "       [0.02635444],\n",
      "       [0.02912672]]), 'dLoss_dZ3': array([[0.0080231 ],\n",
      "       [0.00649567],\n",
      "       [0.00727958]]), 'dZ3_dW3': array([[0.51314355],\n",
      "       [0.4859332 ],\n",
      "       [0.50893877],\n",
      "       [0.47364809]]), 'dZ3_dB3': array([[1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW3': array([[ 0.01128212, -0.00132198,  0.01609135,  0.00394014],\n",
      "       [ 0.01323521,  0.00600506,  0.0031097 , -0.0006397 ],\n",
      "       [ 0.00189936,  0.00286212,  0.00715356,  0.00439192]]), 'dLoss_dB3': array([[0.01277356],\n",
      "       [0.02912613],\n",
      "       [0.02256462]]), 'dZ_3_dZ2': array([[ 0.00216881],\n",
      "       [-0.00056596],\n",
      "       [ 0.00240345],\n",
      "       [-0.00032291]]), 'dLoss_dZ2': array([[ 0.00216881],\n",
      "       [-0.        ],\n",
      "       [ 0.00240345],\n",
      "       [-0.        ]]), 'dZ2_dW2': array([[0.0976316 ],\n",
      "       [0.02795802],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.09218016]]), 'dZ2_dB2': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW2': array([[ 0.00700514, -0.00682466,  0.00550013,  0.00348482,  0.00202283],\n",
      "       [-0.00051394, -0.00207497,  0.00381588,  0.00278332,  0.00384266],\n",
      "       [ 0.00217301, -0.00190425, -0.00082262, -0.00344378, -0.00034018],\n",
      "       [-0.00026903, -0.00029657, -0.00126681, -0.00353748,  0.00440554]]), 'dLoss_dB2': array([[ 0.00656279],\n",
      "       [ 0.00151213],\n",
      "       [ 0.00627786],\n",
      "       [-0.00107455]]), 'dZ_2_dZ1': array([[ 3.87846874e-04],\n",
      "       [-3.93423836e-04],\n",
      "       [ 1.99032127e-04],\n",
      "       [-1.43811296e-05],\n",
      "       [ 5.20690108e-05]]), 'dLoss_dZ1': array([[ 9.67310271e-05],\n",
      "       [-9.83367416e-05],\n",
      "       [ 4.91452800e-05],\n",
      "       [-3.54309572e-06],\n",
      "       [ 1.29896393e-05]]), 'dZ1_dW1': array([[ 0.1],\n",
      "       [-0.2],\n",
      "       [ 0.3],\n",
      "       [-0.4],\n",
      "       [ 0.5]]), 'dZ1_dB1': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'dLoss_dW1': array([[-3.12259595e-03, -4.12168813e-03,  7.58792515e-03,\n",
      "        -2.63360456e-04,  4.64325237e-03],\n",
      "       [ 9.08382947e-04,  2.45681261e-03,  1.91971516e-03,\n",
      "        -4.16166189e-05,  3.86151313e-03],\n",
      "       [-4.17322853e-03,  3.68179447e-03, -3.09145932e-03,\n",
      "         4.69952294e-03,  3.97397557e-04],\n",
      "       [ 7.97604970e-03,  2.87961538e-03, -1.10745624e-02,\n",
      "         4.10752321e-03, -9.94853003e-03],\n",
      "       [ 1.64883782e-03, -1.52953986e-03,  5.62855148e-03,\n",
      "         2.96431075e-03,  3.10562356e-03]]), 'dLoss_dB1': array([[-0.00022366],\n",
      "       [-0.00072199],\n",
      "       [-0.00729678],\n",
      "       [-0.00233376],\n",
      "       [ 0.00214669]])}\n"
     ]
    }
   ],
   "source": [
    "print(apre)\n",
    "{'dLoss_dZ5': array([[-2.69930244]]), 'dZ5_dW5': array([[0.50686782],\n",
    "       [0.50957864],\n",
    "       [0.5237998 ],\n",
    "       [0.49340271],\n",
    "       [0.50839956]]), 'dZ5_dB5': array([[1.]]), 'dLoss_W5': array([[-1.36685405, -1.37821948, -1.40785475, -1.32604112, -1.3688231 ]]), 'dLoss_B5': array([[-2.69136827]]), 'dZ_5_dZ4': array([[-0.07209804],\n",
    "       [ 0.14644256],\n",
    "       [-0.32604005],\n",
    "       [-0.31322776],\n",
    "       [-0.18900884]]), 'dLoss_dZ4': array([[-0.07209804],\n",
    "       [ 0.14644256],\n",
    "       [-0.32604005],\n",
    "       [-0.        ],\n",
    "       [-0.18900884]]), 'dZ4_dW4': array([[0.        ],\n",
    "       [0.23866093],\n",
    "       [0.03396447]]), 'dZ4_dB4': array([[1.],\n",
    "       [1.],\n",
    "       [1.],\n",
    "       [1.],\n",
    "       [1.]]), 'dLoss_dW4': array([[ 1.08630584e-02, -2.34751526e-02, -5.39637656e-03],\n",
    "       [ 2.37754764e-03,  3.64073495e-02,  2.13050052e-03],\n",
    "       [-3.54973215e-03, -8.00294807e-02, -1.17496709e-02],\n",
    "       [ 3.05363177e-03,  5.52675816e-06,  5.89043498e-03],\n",
    "       [-4.67048816e-03, -4.47373189e-02, -1.40374193e-02]]), 'dLoss_dB4': array([[-0.43773464],\n",
    "       [-0.43903962],\n",
    "       [-0.43538887],\n",
    "       [-0.44222529],\n",
    "       [-0.43885428]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a726d145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['W5'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e481ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24982725]\n",
      " [0.24980213]\n",
      " [0.2499201 ]\n",
      " [0.24930558]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(act_derivative(\"sigmoid\", forward[\"Z2\"]), act_derivative(\"sigmoid\", forward[\"Z2\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9d96c07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00561287, -0.01557955, -0.14707524, -0.04781501,  0.04179416])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(f'data/layer1.csv').values[:, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf2923c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine\n",
    "\n",
    "def backward_propagation(nl, nh, Y, lambd, parameters, forward, type:list):\n",
    "    \n",
    "    p = parameters\n",
    "    f = forward\n",
    "    L = nl\n",
    "    g = {}\n",
    "    \n",
    "    HL = f[\"H\"+str(L)] #Final prediction\n",
    "    ZL = f[\"Z\"+str(L)] #Last layer's Z (W^{L}H^{L-1} + B^{L})\n",
    "    g[\"dLoss_Z\"+str(L)]= MSE_grad_wrt_y_pred(Y,HL)*act_derivative(type[-1], ZL)\n",
    "    g[\"dZ\"+str(L)+\"_W\"+str(L)] = f[\"H\"+str(L-1)]\n",
    "    g[\"dZ\"+str(L)+\"_B\"+str(L)]= np.ones((nh[-1],1))\n",
    "    \n",
    "    \n",
    "    g[\"dLoss_W\"+str(L)]=np.dot(g[\"dLoss_Z\"+str(L)], \n",
    "                               g[\"dZ\"+str(L)+\"_W\"+str(L)].T) \\\n",
    "                             + lambd*(p[\"W\"+str(L)])\n",
    "    g[\"dLoss_B\"+str(L)]=np.dot(g[\"dLoss_Z\"+str(L)],\n",
    "                               g[\"dZ\"+str(L)+\"_B\"+str(L)]) \\\n",
    "                             + lambd*(p[\"B\"+str(L)])\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        g[\"dZ_\"+str(l+1)+\"_Z\"+str(l)]= np.dot(act_derivative(type[l],f[\"Z\"+str(l)]),\n",
    "                                              p[\"W\"+str(l+1)].T)\n",
    "        \n",
    "        g[\"dLoss_Z\"+str(l)]= np.dot(g[\"dLoss_Z\"+str(l+1)],g[\"dZ_\"+str(l+1)+\"_Z\"+str(l)])\n",
    "        \n",
    "        \n",
    "        g[\"dZ\"+str(l)+\"_W\"+str(l)] = f[\"H\" + str(l-1)]\n",
    "        g[\"dZ\"+str(l)+\"_B\"+str(l)]= np.ones((nh[l],1))\n",
    "\n",
    "        \n",
    "        g[\"dLoss_W\"+str(l)]= np.dot(g[\"dLoss_Z\"+str(l)],g[\"dZ\" +str(l)+ \"_W\" +str(l)].T) \\\n",
    "                                    + lambd*(p[\"W\" + str(l)])\n",
    "        g[\"dLoss_B\"+str(l)] = np.dot(g[\"dLoss_Z\"+str(l)],g[\"dZ\" +str(l)+ \"_B\"+str(l)]) \\\n",
    "                                    + lambd*(p[\"B\" + str(l)])\n",
    "    return g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
