{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008afd68",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6cffaa",
   "metadata": {},
   "source": [
    "_Back ground functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b20f50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the activation function\n",
    "def act_func(x, type):\n",
    "    \"\"\"\n",
    "    Compute the activation function for a given type.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - type (str): Type of activation function ('sigmoid' or 'ReLU').\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Output of the activation function.\n",
    "    \"\"\"\n",
    "    if type == \"sigmoid\":\n",
    "        # Compute the sigmoid activation function: 1 / (1 + exp(-x))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    if type == \"ReLU\":\n",
    "        # Compute the ReLU activation function: max(0, x)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    \n",
    "#get H's [sigma(WX + b)] and the Z's [WX + b]\n",
    "def feed_forward(X, nl, act:list, parameters: dict):\n",
    "    \"\"\"\n",
    "    Perform the feedforward pass through the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): Input data.\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - act (list): List of activation functions for each layer.\n",
    "    - parameters (dict): Dictionary containing the parameters of the neural network.\n",
    "\n",
    "    Returns:\n",
    "    - forward: Dictionary containing the forward pass computations (ZL and HL)\n",
    "    \"\"\"\n",
    "    p = parameters\n",
    "    forward = {}\n",
    "    forward[\"H0\"] = X  # Input layer is the initial value of H0\n",
    "    L = nl \n",
    "    for l in range(1, L + 1):\n",
    "        # Calculate the linear transformation Zl = Wl * Hl-1 + Bl\n",
    "        forward[\"Z\" + str(l)] = np.dot(p[\"W\" + str(l)], forward[\"H\" + str(l - 1)]) \\\n",
    "                               + p[\"B\" + str(l)]\n",
    "        # Apply the activation function to compute Hl\n",
    "        forward[\"H\" + str(l)] = act_func(forward[\"Z\" + str(l)], act[l-1])\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0dfefe",
   "metadata": {},
   "source": [
    "### Part 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_pred, lambd: float, parameters: list):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) loss function \n",
    "    with L2 penalty (Ridge Regression).\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The true target values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "    - lambd (float): The regularization parameter for the L2 penalty.\n",
    "    - parameters (dict): A dictionary containing the parameters of the neural network.\n",
    "\n",
    "    Returns:\n",
    "    - MSE (float): The MSE loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    # Calculate L2 penalty (sum of squares of all parameters)\n",
    "    # For each layer, square the parameters and then sum their sums\n",
    "    # parameters.values() returns the value for each key in the dict\n",
    "    L2_penalty = np.sum([np.sum(param**2) for param in parameters.values()])\n",
    "    # Compute MSE with L2 penalty\n",
    "    MSE = (1/2) * np.linalg.norm(y - y_pred)**2 + lambd * L2_penalty\n",
    "    return MSE\n",
    "\n",
    "def Cross_entropy(y, y_pred, lambd: float, parameters: list):\n",
    "    \"\"\"\n",
    "    TO DO: Compute the cross entropy loss\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd5c24",
   "metadata": {},
   "source": [
    "### Part 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d87c8",
   "metadata": {},
   "source": [
    "__To do__... math derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "765f0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad_wrt_y_pred(y,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the Mean Squared Error (MSE) loss function \n",
    "    with respect to the predicted value (last output of neural network)\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The true target values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of the MSE loss with respect to the predicted values.\n",
    "    \"\"\"\n",
    "    mse_grad = (y_pred - y)\n",
    "    return mse_grad\n",
    "\n",
    "\n",
    "def Cross_entrpy_grad_wrt_y_pred(y,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the cross entropy loss with L2 penalty \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f58de",
   "metadata": {},
   "source": [
    "### Part 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_derivative(type, Zl):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the activation function with respect to its input.\n",
    "    \n",
    "    Parameters:\n",
    "    - activation (str): Type of activation function ('sigmoid' or 'ReLU').\n",
    "    - Zl (numpy.ndarray): Input array to the activation function (pre-activation values).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Derivative of the activation function evaluated at Zl.\n",
    "    \"\"\"\n",
    "    if type == \"sigmoid\":\n",
    "        # Compute the sigmoid of Zl\n",
    "        sigmoid = 1 / (1 + np.exp(-Zl))\n",
    "        # Compute the derivative of the sigmoid function\n",
    "        derivative = sigmoid * (1 - sigmoid)\n",
    "        return derivative\n",
    "    \n",
    "    if type == \"ReLU\":\n",
    "        # Compute derivative: 1 when input > 0, 0 otherwise\n",
    "        derivative = np.where(Zl > 0, 1, 0)\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73b54a",
   "metadata": {},
   "source": [
    "### Part 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6faff6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine updated with doc string and comments\n",
    "def backward_propagation(nl, nh, Y, lambd, p, forward, act:list):\n",
    "    \"\"\"\n",
    "    Perform backward propagation for a neural network to compute gradients \n",
    "    for all parameters.\n",
    "\n",
    "    Args:\n",
    "    nl (int): Number of layers in the neural network.\n",
    "    nh (list): an int vec of length nl - that has the number of neurons in each layer\n",
    "    Y (array): Observed values we try to predict\n",
    "    lambd (float): Regularization parameter.\n",
    "    parameters (dict): Dict containing parameters 'W' (weights) and 'B' (biases)\n",
    "    forward (dict): Dictionary containing the forward pass computations (ZL and HL).\n",
    "    act (list):a str vec of length nl - the activation function used in each layer \n",
    "\n",
    "    Returns:\n",
    "    g: A dictionary containing gradients of loss with respect to each parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = p\n",
    "    f = forward\n",
    "    L = nl\n",
    "    g = {}\n",
    "    \n",
    "    HL = f[\"H\"+str(L)] #Final prediction sigma(W^{L}H^{L-1} + B^{L})\n",
    "    ZL = f[\"Z\"+str(L)] #Last layer's Z (W^{L}H^{L-1} + B^{L})\n",
    "    \n",
    "    # Compute gradient of loss wrt last layer Z (dL_dHL*dHL_dZL)\n",
    "    g[\"dLoss_dZ\"+str(L)] = MSE_grad_wrt_y_pred(Y, HL) * act_derivative(act[-1], ZL) \n",
    "    \n",
    "    # Deriative of last layer Z wrt its weights & biases (dZL_dWL, dZL_dBL)\n",
    "    g[\"dZ\"+str(L)+\"_dW\"+str(L)] = f[\"H\"+str(L-1)] #just H^{L-1}\n",
    "    g[\"dZ\"+str(L)+\"_dB\"+str(L)] = np.ones((nh[-1], 1))#vec of 1's row's of last layer neurons\n",
    "    \n",
    "    # Calculate derivative with respect to weights and biases for the last layer\n",
    "    # dLoss_dWL = dL_dHL*dHL_dZL*dZL_dWL, dLoss_dBL = dL_dHL*dHL_dZL*dZL_dBL\n",
    "    g[\"dLoss_W\"+str(L)] = np.dot(g[\"dLoss_dZ\"+str(L)], \n",
    "                                 g[\"dZ\"+str(L)+\"_dW\"+str(L)].T) \\\n",
    "                           + lambd * p[\"W\"+str(L)]  # Include regularization term\n",
    "    g[\"dLoss_B\"+str(L)] = np.dot(g[\"dLoss_dZ\"+str(L)],\n",
    "                                 g[\"dZ\"+str(L)+\"_dB\"+str(L)]) \\\n",
    "                           + lambd * p[\"B\"+str(L)]  # Include regularization term\n",
    "    \n",
    "    #from L-1 to first layer, which is 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        # Calculate gradient of Z in layer l+1 wrt Z in layer l\n",
    "        # dZl+1_dZl = dZl+1_dHl*dHl_dZl \n",
    "        g[\"dZ_\"+str(l+1)+\"_dZ\"+str(l)] = np.dot(act_derivative(act[l], f[\"Z\"+str(l)]),\n",
    "                                                p[\"W\"+str(l+1)].T)\n",
    "        \n",
    "        # Propagate the loss gradient back from layer l+1 to layer l\n",
    "        #dLoss_dZl = dLoss_dZl+1*dZl+1_dHl*dHl_dZl\n",
    "        g[\"dLoss_dZ\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l+1)], \n",
    "                                      g[\"dZ_\"+str(l+1)+\"_dZ\"+str(l)])\n",
    "        \n",
    "        # Deriative of Z wrt its weights & biases (for each layer)\n",
    "        g[\"dZ\"+str(l)+\"_dW\"+str(l)] = f[\"H\" + str(l-1)]\n",
    "        g[\"dZ\"+str(l)+\"_dB\"+str(l)] = np.ones((nh[l], 1))\n",
    "\n",
    "        # Calculate derivatives with respect to weights and biases in layer l\n",
    "        g[\"dLoss_dW\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l)], \n",
    "                                     g[\"dZ\"+str(l)+ \"_dW\"+str(l)].T) \\\n",
    "                               + lambd * p[\"W\" + str(l)]\n",
    "        g[\"dLoss_dB\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l)], \n",
    "                                     g[\"dZ\"+str(l)+ \"_dB\"+str(l)]) \\\n",
    "                               + lambd * p[\"B\" + str(l)]\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa7f20",
   "metadata": {},
   "source": [
    "### Part 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04471e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter(nl, nh):\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) of a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - nh (list): List containing the number of neurons in each layer.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing the initialized parameters.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for n in range(1, nl + 1):\n",
    "        # Initialize weights randomly from a uniform distribution\n",
    "        parameters[\"W\" + str(n)] = np.random.rand(nh[n], nh[n-1])\n",
    "        # Initialize biases as zeros\n",
    "        parameters[\"B\" + str(n)] = np.zeros((nh[n], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 5\n",
    "X = np.array([0.1, -0.2, 0.3, -0.4, 0.5]).reshape(-1, 1)\n",
    "nh = [X.shape[0], 5, 4, 3, 5, 1]\n",
    "act = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393cd41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = np.array([1, 2, 3, 0, 1 , -1, -2 , -34])\n",
    "np.where(sigma < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8d7095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.35"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = np.array([1, 1, 3, 4])\n",
    "\n",
    "#np.sum(parameters**2)\n",
    "\n",
    "#np.square(parameters)\n",
    "#0.05*np.sum(parameters**2)\n",
    "0.05*np.sum(np.square(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d693dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\n",
    "    np.array([[1, 1, 3, 4],  # First matrix\n",
    "              [1, 2, 3, 4]]),\n",
    "    np.array([[2, 1, 3, 4],  # Second matrix\n",
    "              [1, 5, 3, 4]]),\n",
    "    np.array([1, 2])         # Vector\n",
    "]\n",
    "\n",
    "np.sum([np.sum(param) for param in parameters])\n",
    "#for param in parameters:\n",
    "#    print(np.sum(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135e4627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dili/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,4) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,4) into shape (2,)"
     ]
    }
   ],
   "source": [
    "np.sum(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "370cd849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 1)\n",
      "(4, 5)\n",
      "(4, 1)\n",
      "(3, 4)\n",
      "(3, 1)\n",
      "(5, 3)\n",
      "(5, 1)\n",
      "(1, 5)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "for parm in parameters.values():\n",
    "    print(parm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84eb0f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "B1\n",
      "W2\n",
      "B2\n",
      "W3\n",
      "B3\n",
      "W4\n",
      "B4\n",
      "W5\n",
      "B5\n"
     ]
    }
   ],
   "source": [
    "for parm in parameters:\n",
    "    print(parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f8c0ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "268162a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(nh[-2]).reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "594cb8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in reversed(range(1, 5)):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "babd64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2800d3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((nh[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "428ed54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((nh[-2], 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d61a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3ff225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine\n",
    "\n",
    "def backward_propagation(nl, nh, Y, lambd, parameters, forward, type:list):\n",
    "    \n",
    "    p = parameters\n",
    "    f = forward\n",
    "    L = nl\n",
    "    g = {}\n",
    "    \n",
    "    HL = f[\"H\"+str(L)] #Final prediction\n",
    "    ZL = f[\"Z\"+str(L)] #Last layer's Z (W^{L}H^{L-1} + B^{L})\n",
    "    g[\"dLoss_Z\"+str(L)]= MSE_grad_wrt_y_pred(Y,HL)*act_derivative(type[-1], ZL)\n",
    "    g[\"dZ\"+str(L)+\"_W\"+str(L)] = f[\"H\"+str(L-1)]\n",
    "    g[\"dZ\"+str(L)+\"_B\"+str(L)]= np.ones((nh[-1],1))\n",
    "    \n",
    "    \n",
    "    g[\"dLoss_W\"+str(L)]=np.dot(g[\"dLoss_Z\"+str(L)], \n",
    "                               g[\"dZ\"+str(L)+\"_W\"+str(L)].T) \\\n",
    "                             + lambd*(p[\"W\"+str(L)])\n",
    "    g[\"dLoss_B\"+str(L)]=np.dot(g[\"dLoss_Z\"+str(L)],\n",
    "                               g[\"dZ\"+str(L)+\"_B\"+str(L)]) \\\n",
    "                             + lambd*(p[\"B\"+str(L)])\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        g[\"dZ_\"+str(l+1)+\"_Z\"+str(l)]= np.dot(act_derivative(type[l],f[\"Z\"+str(l)]),\n",
    "                                              p[\"W\"+str(l+1)].T)\n",
    "        \n",
    "        g[\"dLoss_Z\"+str(l)]= np.dot(g[\"dLoss_Z\"+str(l+1)],g[\"dZ_\"+str(l+1)+\"_Z\"+str(l)])\n",
    "        \n",
    "        \n",
    "        g[\"dZ\"+str(l)+\"_W\"+str(l)] = f[\"H\" + str(l-1)]\n",
    "        g[\"dZ\"+str(l)+\"_B\"+str(l)]= np.ones((nh[l],1))\n",
    "\n",
    "        \n",
    "        g[\"dLoss_W\"+str(l)]= np.dot(g[\"dLoss_Z\"+str(l)],g[\"dZ\" +str(l)+ \"_W\" +str(l)].T) \\\n",
    "                                    + lambd*(p[\"W\" + str(l)])\n",
    "        g[\"dLoss_B\"+str(l)] = np.dot(g[\"dLoss_Z\"+str(l)],g[\"dZ\" +str(l)+ \"_B\"+str(l)]) \\\n",
    "                                    + lambd*(p[\"B\" + str(l)])\n",
    "    return g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
