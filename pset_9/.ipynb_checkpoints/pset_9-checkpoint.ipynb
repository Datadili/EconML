{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3768ee02",
   "metadata": {},
   "source": [
    "# Dili Maduabum, Josh Bailey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c099ab6",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053fa185",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbba677",
   "metadata": {},
   "source": [
    "Consider a typical neural network setup, where each layer's output, $ h^l $, is computed using a weight matrix $ W^l $, a bias $ b^l $, and an activation function $ f^l $. \n",
    "\n",
    "Given that the output of layer $ l $ is defined by:\n",
    "$$ h^l = f^l(W^l h^{l-1} + b^l) $$\n",
    "where $ f^l $ is the activation function applied element-wise.\n",
    "\n",
    "To show that:\n",
    "$$ \\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "we can use the chain rule:\n",
    "\n",
    "1. Start by noting the function composition in $ h^l $:\n",
    "   $$ h^l = f^l(z^l) $$\n",
    "   where $ z^l = W^l h^{l-1} + b^l $.\n",
    "\n",
    "2. The derivative of $ h^l $ with respect to $ b^l $ is:\n",
    "   $$ \\frac{\\partial h^l}{\\partial b^l} = \\frac{\\partial f^l}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial b^l} $$\n",
    "   \n",
    "   Applying the chain rule, we have:\n",
    "$$\\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}\\left(W^l h^{l-1} + b^l\\right) \\cdot \\frac{\\partial (W^l h^{l-1} + b^l)}{\\partial b^l}$$\n",
    "\n",
    "3. Since $ z^l $ is linear in $ b^l $, the derivative $ \\frac{\\partial z^l}{\\partial b^l} $ is straightforwardly 1 (keeping in mind that $ b^l $ is added element-wise to $ W^l h^{l-1} $).\n",
    "\n",
    "4. Therefore, $ \\frac{\\partial h^l}{\\partial b^l} $ simplifies to:\n",
    "   $$ \\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}(z^l) $$\n",
    "   which is exactly:\n",
    "   $$ \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c8458",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563691d",
   "metadata": {},
   "source": [
    "Compute the gradient of the loss function $ L $ with respect to the biases $ b^l $ using the chain rule in the context of backpropagation. \n",
    "\n",
    "1. Recognize that the gradient of the loss $ L $ with respect to the biases $ b^l $ can be propagated from the output backwards using the chain rule:\n",
    "   $$ \\frac{\\partial L}{\\partial b^l} = \\frac{\\partial L}{\\partial h^l} \\cdot \\frac{\\partial h^l}{\\partial b^l} $$\n",
    "\n",
    "2. From Part 1, we know:\n",
    "   $$ \\frac{\\partial h^l}{\\partial b^l} = \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "\n",
    "3. The derivative $ \\frac{\\partial L}{\\partial h^l} $ is calculated during backpropagation as the \"upstream gradient\" from later layers, often denoted as $ \\delta^l $. It represents how the change in $ h^l $ impacts the change in loss.\n",
    "\n",
    "4. Combining these, the formula for $ \\frac{\\partial L}{\\partial b^l} $ in terms of the gradients from later stages in the network is:\n",
    "   $$ \\frac{\\partial L}{\\partial b^l} = \\delta^l \\odot \\left(f^l\\right)^{\\prime}(W^l h^{l-1} + b^l) $$\n",
    "   where $ \\odot $ denotes the element-wise product.\n",
    "\n",
    "By backpropagating the gradient from the output layer back to the inputs using these steps, we ensure that the biases $ b^l $ are updated in a way that minimizes the loss $ L $, thereby improving the model's performance with each training iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c36a3",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eb67b",
   "metadata": {},
   "source": [
    "_Back ground functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "41e08b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the activation function\n",
    "def act_func(x, type):\n",
    "    \"\"\"\n",
    "    Compute the activation function for a given type.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - type (str): Type of activation function ('sigmoid' or 'ReLU').\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Output of the activation function.\n",
    "    \"\"\"\n",
    "    if type == \"sigmoid\":\n",
    "        # Compute the sigmoid activation function: 1 / (1 + exp(-x))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    if type == \"ReLU\":\n",
    "        # Compute the ReLU activation function: max(0, x)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    \n",
    "#get H's [sigma(WX + b)] and the Z's [WX + b]\n",
    "def feed_forward(X, nl, act:list, parameters: dict):\n",
    "    \"\"\"\n",
    "    Perform the feedforward pass through the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): Input data.\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - act (list): List of activation functions for each layer.\n",
    "    - parameters (dict): Dictionary containing the parameters of the neural network.\n",
    "\n",
    "    Returns:\n",
    "    - forward: Dictionary containing the forward pass computations (ZL and HL)\n",
    "    \"\"\"\n",
    "    p = parameters\n",
    "    forward = {}\n",
    "    forward[\"H0\"] = X.T  # Input layer is the initial value of H0\n",
    "    L = nl \n",
    "    for l in range(1, L + 1):\n",
    "        # Calculate the linear transformation Zl = Wl * Hl-1 + Bl\n",
    "        forward[\"Z\" + str(l)] = np.dot(p[\"W\" + str(l)], forward[\"H\" + str(l - 1)]) + p[\"B\" + str(l)]\n",
    "\n",
    "        # Apply the activation function to compute Hl\n",
    "        forward[\"H\" + str(l)] = act_func(forward[\"Z\" + str(l)], act[l-1])\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbf9c8",
   "metadata": {},
   "source": [
    "### Part 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "4f45d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_pred, lambd: float, parameters: list):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) loss function \n",
    "    with L2 penalty (Ridge Regression).\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The true target values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "    - lambd (float): The regularization parameter for the L2 penalty.\n",
    "    - parameters (dict): A dictionary containing the parameters of the neural network.\n",
    "\n",
    "    Returns:\n",
    "    - MSE (float): The MSE loss with L2 penalty.\n",
    "    \"\"\"\n",
    "    # Calculate L2 penalty (sum of squares of all parameters)\n",
    "    # For each layer, square the parameters and then sum their sums\n",
    "    # parameters.values() returns the value for each key in the dict\n",
    "    L2_penalty = np.sum([np.sum(param**2) for param in parameters.values()])\n",
    "    # Compute MSE with L2 penalty\n",
    "    MSE = (1/2) * np.linalg.norm(y - y_pred)**2 + lambd * L2_penalty\n",
    "    return MSE\n",
    "\n",
    "def Cross_entropy(y, y_pred, lambd: float, parameters: list):\n",
    "    \"\"\"\n",
    "    TO DO: Compute the cross entropy loss\n",
    "    \"\"\"\n",
    "    Cross_entropy = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    L2_penalty=np.sum([np.sum(param**2) for param in parameters.values()])\n",
    "    return Cross_entropy +  L2_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15518c4d",
   "metadata": {},
   "source": [
    "### Part 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50b850",
   "metadata": {},
   "source": [
    "The MSE loss function is given by:\n",
    "$$ \\text{MSE} = \\frac{1}{2} \\sum (y - \\hat{y})^2 $$\n",
    "\n",
    "To find the derivative with respect to the network outputs $ \\hat{y} $, we have:\n",
    "$$ \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\hat{y} - y $$\n",
    "\n",
    "The derivative of $ x^2 $ is $ 2x $, and the $ \\frac{1}{2} $ factor cancels out the 2 from the derivative.\n",
    "\n",
    "The cross-entropy loss function for two classes is given by:\n",
    "$$ \\text{Cross-Entropy} = -\\left( y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}) \\right) $$\n",
    "\n",
    "The derivative with respect to the network outputs $ \\hat{y} $ is:\n",
    "$$ \\frac{\\partial \\text{Cross-Entropy}}{\\partial \\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}} \\right) $$\n",
    "This can be simplified to:\n",
    "$$ \\frac{\\partial \\text{Cross-Entropy}}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})} $$\n",
    "\n",
    "For a network using a softmax output layer (multiclass classification), the derivative simplifies to $ \\hat{y} - y $ because the softmax output assumes the entire output vector sums to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b37ced59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad_wrt_y_pred(y,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the Mean Squared Error (MSE) loss function \n",
    "    with respect to the predicted value (last output of neural network)\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The true target values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of the MSE loss with respect to the predicted values.\n",
    "    \"\"\"\n",
    "    mse_grad = (y_pred - y)\n",
    "    return mse_grad\n",
    "\n",
    "\n",
    "def CE_grad_wrt_y_pred(y,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the cross entropy loss with L2 penalty \n",
    "    \"\"\"\n",
    "    cross_entrpy_grad = (y_pred - y) / (y_pred * (1 - y_pred)) \n",
    "    return cross_entrpy_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf2a6d",
   "metadata": {},
   "source": [
    "### Part 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "8b883e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_derivative(type, Zl):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the activation function with respect to its input.\n",
    "    \n",
    "    Parameters:\n",
    "    - activation (str): Type of activation function ('sigmoid' or 'ReLU').\n",
    "    - Zl (numpy.ndarray): Input array to the activation function (pre-activation values).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Derivative of the activation function evaluated at Zl.\n",
    "    \"\"\"\n",
    "    if type == \"sigmoid\":\n",
    "        # Compute the sigmoid of Zl\n",
    "        sigmoid = 1 / (1 + np.exp(-Zl))\n",
    "        # Compute the derivative of the sigmoid function\n",
    "        derivative = sigmoid * (1 - sigmoid)\n",
    "        return derivative\n",
    "    \n",
    "    if type == \"ReLU\":\n",
    "        # Compute derivative: 1 when input > 0, 0 otherwise\n",
    "        derivative = np.where(Zl > 0, 1, 0)\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308347f4",
   "metadata": {},
   "source": [
    "### Part 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "e1a4a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine updated with doc string and comments\n",
    "def backward_propagation(loss, nl, nh, Y, lambd, parameters, forward, act:list):\n",
    "    \"\"\"\n",
    "    Perform backward propagation for a neural network to compute gradients \n",
    "    for all parameters.\n",
    "\n",
    "    Args:\n",
    "    nl (int): Number of layers in the neural network.\n",
    "    nh (list): an int vec of length nl - that has the number of neurons in each layer\n",
    "    Y (array): Observed values we try to predict\n",
    "    lambd (float): Regularization parameter.\n",
    "    parameters (dict): Dict containing parameters 'W' (weights) and 'B' (biases)\n",
    "    forward (dict): Dictionary containing the forward pass computations (ZL and HL).\n",
    "    act (list):a str vec of length nl - the activation function used in each layer \n",
    "\n",
    "    Returns:\n",
    "    g (dict): A dictionary containing gradients of loss with respect to each parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = parameters\n",
    "    f = forward\n",
    "    L = nl\n",
    "    g = {}\n",
    "    \n",
    "    HL = f[\"H\"+str(L)] #Final prediction sigma(W^{L}H^{L-1} + B^{L})\n",
    "    ZL = f[\"Z\"+str(L)] #Last layer's Z (W^{L}H^{L-1} + B^{L})\n",
    "    \n",
    "    if loss == \"MSE\":\n",
    "        # Compute gradient of loss wrt last layer Z (dL_dHL*dHL_dZL)\n",
    "        g[\"dLoss_dZ\"+str(L)] = MSE_grad_wrt_y_pred(Y, HL) * act_derivative(act[-1], ZL) \n",
    "    if loss == \"CE\":\n",
    "         g[\"dLoss_dZ\"+str(L)] = CE_grad_wrt_y_pred(Y, HL) * act_derivative(act[-1], ZL) \n",
    "    # Deriative of last layer Z wrt its weights & biases (dZL_dWL, dZL_dBL)\n",
    "    g[\"dZ\"+str(L)+\"_dW\"+str(L)] = f[\"H\"+str(L-1)] #just H^{L-1}\n",
    "    g[\"dZ\"+str(L)+\"_dB\"+str(L)] = np.ones((nh[-1], 1))#vec of 1's row's of last layer neurons\n",
    "    \n",
    "    # Calculate derivative with respect to weights and biases for the last layer\n",
    "    # dLoss_dWL = dL_dHL*dHL_dZL*dZL_dWL, dLoss_dBL = dL_dHL*dHL_dZL*dZL_dBL\n",
    "    g[\"dLoss_dW\"+str(L)] = np.dot(g[\"dLoss_dZ\"+str(L)], \n",
    "                                 g[\"dZ\"+str(L)+\"_dW\"+str(L)].T) \\\n",
    "                           + lambd * p[\"W\"+str(L)]  # Include regularization term\n",
    "    g[\"dLoss_dB\"+str(L)] = np.dot(g[\"dLoss_dZ\"+str(L)].T,\n",
    "                                 g[\"dZ\"+str(L)+\"_dB\"+str(L)]) \\\n",
    "                           + lambd * p[\"B\"+str(L)]  # Include regularization term\n",
    "    \n",
    "    #from L-1 to first layer, which is 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        # Calculate gradient of Z in layer l+1 wrt Z in layer l\n",
    "        # dZl+1_dZl = dZl+1_dHl*dHl_dZl \n",
    "        g[\"dZ_\"+str(l+1)+\"_dZ\"+str(l)] = np.dot(p[\"W\"+str(l+1)].T, \n",
    "                                                g[\"dLoss_dZ\"+str(l+1)])\n",
    "        \n",
    "        # Propagate the loss gradient back from layer l+1 to layer l\n",
    "        #dLoss_dZl = dLoss_dZl+1*dZl+1_dHl*dHl_dZl\n",
    "        g[\"dLoss_dZ\"+str(l)] = g[\"dZ_\"+str(l+1)+\"_dZ\"+str(l)] * \\\n",
    "                                act_derivative(act[l], f[\"Z\"+str(l)])\n",
    "        \n",
    "        # Deriative of Z wrt its weights & biases (for each layer)\n",
    "        g[\"dZ\"+str(l)+\"_dW\"+str(l)] = f[\"H\" + str(l-1)]\n",
    "        g[\"dZ\"+str(l)+\"_dB\"+str(l)] = np.ones((nh[l], 1))\n",
    "\n",
    "        # Calculate derivatives with respect to weights and biases in layer l\n",
    "        g[\"dLoss_dW\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l)], \n",
    "                                     g[\"dZ\"+str(l)+ \"_dW\"+str(l)].T) \\\n",
    "                               + lambd * p[\"W\" + str(l)]\n",
    "        g[\"dLoss_dB\"+str(l)] = np.dot(g[\"dLoss_dZ\"+str(l)].T,\n",
    "                                      g[\"dZ\"+str(l)+ \"_dB\"+str(l)]) \\\n",
    "                                + lambd * p[\"B\" + str(l)]\n",
    "                                \n",
    "        \n",
    "                              \n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3f023",
   "metadata": {},
   "source": [
    "### Part 3b\n",
    "\n",
    "_loading the neccesities from pset 8_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "c4caea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = 5\n",
    "X = np.array([0.1, -0.2, 0.3, -0.4, 0.5]).reshape(1, -1)\n",
    "nh = [X.shape[1], 5, 4, 3, 5, 1]\n",
    "act = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"] \n",
    "Y = 3\n",
    "\n",
    "def parameterss(nl):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    \n",
    "    Returns:\n",
    "    - parameters (dict): Dictionary containing all the weights and biases\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for l in range(1, nl + 1): #1 to 5\n",
    "        # Select the values all but the last one (which is the bias)\n",
    "        parameters[\"W\" + str(l)] = pd.read_csv(f'data/layer{l}.csv').values[:, :-1]\n",
    "        # Select the values the last one only (the bias), and make it a column vector\n",
    "        parameters[\"B\" + str(l)] = pd.read_csv(f'data/layer{l}.csv').values[:, -1].reshape(-1,1)\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b0b69",
   "metadata": {},
   "source": [
    "_computing the back prop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "dc216761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (MSE) of the weights in respect to Weights of the first hidden layer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.062636</td>\n",
       "      <td>-0.082066</td>\n",
       "      <td>0.151207</td>\n",
       "      <td>-0.004532</td>\n",
       "      <td>0.091946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.048763</td>\n",
       "      <td>0.038955</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>0.078164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.083558</td>\n",
       "      <td>0.073823</td>\n",
       "      <td>-0.062109</td>\n",
       "      <td>0.094364</td>\n",
       "      <td>0.007481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159528</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>-0.221471</td>\n",
       "      <td>0.082124</td>\n",
       "      <td>-0.198937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032952</td>\n",
       "      <td>-0.030541</td>\n",
       "      <td>0.112497</td>\n",
       "      <td>0.059385</td>\n",
       "      <td>0.061989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0 -0.062636 -0.082066  0.151207 -0.004532  0.091946\n",
       "1  0.018354  0.048763  0.038955 -0.001580  0.078164\n",
       "2 -0.083558  0.073823 -0.062109  0.094364  0.007481\n",
       "3  0.159528  0.057579 -0.221471  0.082124 -0.198937\n",
       "4  0.032952 -0.030541  0.112497  0.059385  0.061989"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (MSE)of the bias in respect to Bias of the first hidden layer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.147018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.047758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.005556\n",
       "1 -0.015523\n",
       "2 -0.147018\n",
       "3 -0.047758\n",
       "4  0.041851"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (Cross Entropy) of the weights in respect to Weights of the first hidden layer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.062599</td>\n",
       "      <td>-0.082139</td>\n",
       "      <td>0.151316</td>\n",
       "      <td>-0.004677</td>\n",
       "      <td>0.092128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018318</td>\n",
       "      <td>0.048836</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.001432</td>\n",
       "      <td>0.077980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.083539</td>\n",
       "      <td>0.073786</td>\n",
       "      <td>-0.062054</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.007573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159526</td>\n",
       "      <td>0.057582</td>\n",
       "      <td>-0.221475</td>\n",
       "      <td>0.082129</td>\n",
       "      <td>-0.198944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032957</td>\n",
       "      <td>-0.030551</td>\n",
       "      <td>0.112512</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>0.062013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0 -0.062599 -0.082139  0.151316 -0.004677  0.092128\n",
       "1  0.018318  0.048836  0.038844 -0.001432  0.077980\n",
       "2 -0.083539  0.073786 -0.062054  0.094290  0.007573\n",
       "3  0.159526  0.057582 -0.221475  0.082129 -0.198944\n",
       "4  0.032957 -0.030551  0.112512  0.059365  0.062013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (Cross Entropy) of the weights in respect to Bias of the first hidden layer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.146804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.047544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.005342\n",
       "1 -0.015309\n",
       "2 -0.146804\n",
       "3 -0.047544\n",
       "4  0.042065"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = parameterss(nl)\n",
    "forward = feed_forward(X, nl, act, parameters)\n",
    "lambd = 1 #random since now was given\n",
    "\n",
    "grads_mse = backward_propagation(\"MSE\", nl, nh, Y, lambd, parameters, forward, act)\n",
    "\n",
    "print(\"Gradient (MSE) of the weights in respect to Weights of the first hidden layer:\")\n",
    "display(pd.DataFrame(grads_mse['dLoss_dW1']))\n",
    "print(\"Gradient (MSE)of the bias in respect to Bias of the first hidden layer:\")\n",
    "display(pd.DataFrame(grads_mse['dLoss_dB1']))\n",
    "\n",
    "grads_ce = backward_propagation(\"CE\", nl, nh, Y, lambd, parameters, forward, act)\n",
    "\n",
    "print(\"Gradient (Cross Entropy) of the weights in respect to Weights of the first hidden layer:\")\n",
    "display(pd.DataFrame(grads_ce['dLoss_dW1']))\n",
    "print(\"Gradient (Cross Entropy) of the weights in respect to Bias of the first hidden layer:\")\n",
    "display(pd.DataFrame(grads_ce['dLoss_dB1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b946db2",
   "metadata": {},
   "source": [
    "*Appendix, Finding the dimensions of each*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "101970e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dLoss_dZ5: (1, 1)\n",
      "shape of dLoss_dW5: (1, 5)\n",
      "shape of dLoss_dB5: (1, 1) \n",
      "\n",
      "shape of dLoss_dZ4: (5, 1)\n",
      "shape of dLoss_dW4: (5, 3)\n",
      "shape of dLoss_dB4: (5, 1) \n",
      "\n",
      "shape of dLoss_dZ3: (3, 1)\n",
      "shape of dLoss_dW3: (3, 4)\n",
      "shape of dLoss_dB3: (3, 1) \n",
      "\n",
      "shape of dLoss_dZ2: (4, 1)\n",
      "shape of dLoss_dW2: (4, 5)\n",
      "shape of dLoss_dB2: (4, 1) \n",
      "\n",
      "shape of dLoss_dZ1: (5, 1)\n",
      "shape of dLoss_dW1: (5, 5)\n",
      "shape of dLoss_dB1: (5, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(1, nl  + 1)):\n",
    "    print(\"shape of dLoss_dZ\" +str(i) + \":\", grads_mse[\"dLoss_dZ\"+str(i)].shape)\n",
    "    print(\"shape of dLoss_dW\" +str(i) + \":\", grads_mse[\"dLoss_dW\"+str(i)].shape)\n",
    "    print(\"shape of dLoss_dB\" +str(i) + \":\", grads_mse[\"dLoss_dB\"+str(i)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdb8f4",
   "metadata": {},
   "source": [
    "### Part 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0584cf",
   "metadata": {},
   "source": [
    "Early stopping helps avoid overfitting when training a machine learning model, particularly with iterative methods like stochastic gradient descent. The concept involves monitoring the model's performance on a validation set during training and stopping the training process if the performance on the validation set ceases to improve or begins to degrade, despite continued improvements on the training set.\n",
    "\n",
    "Early stopping effectively adds a regularization effect without the need for explicit regularization techniques like $ L^1 $ or $ L^2 $ penalties. It prevents the model from learning noise and complex patterns in the training data that do not generalize to new data, which is essentially what overfitting is. It can also reduce computational costs by stopping training before all allocated resources (like epochs) are used, especially if no further learning improvement is detected.\n",
    "\n",
    "While helpful, calling it a \"free lunch\" should be taken with caution. It doesn’t avoid the need to deal with needing to tune of the patience parameter and set up the validation process to avoid premature stopping or too late stopping, which can still lead to underfitting or overfitting, respectively. Moreover, it relies heavily on having a well-separated validation set that accurately represents unseen data, which isn't always available or easy to construct, especially in cases with limited data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a65c8",
   "metadata": {},
   "source": [
    "### Part 4b\n",
    "\n",
    "_New parameter funcction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2323c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/ps9.csv')\n",
    "Y = data.iloc[:, 0].values.reshape(-1, 1)  # Assuming the first column is the target\n",
    "X = data.iloc[:, 1:].values  # Remaining columns are features\n",
    "\n",
    "# Define neural network structure\n",
    "nl = 5  # Number of layers\n",
    "nh = [10, 20, 15, 10, 5]  # Example: Number of neurons in each layer\n",
    "act = [\"ReLU\", \"sigmoid\", \"ReLU\", \"sigmoid\", \"ReLU\"]  # Activation functions\n",
    "\n",
    "parameters = parameter(nl, nh, X) # Initialize parameters\n",
    "\n",
    "\n",
    "# Set training parameters\n",
    "batch_size = 100\n",
    "patience = 10\n",
    "lambd = 1  # Regularization strength\n",
    "learning_rate = 0.01  # Learning rate for SGD updates\n",
    "\n",
    "\n",
    "def stoch_grad_des_w_early_stop(loss, nh, X, Y, parameters, act, batch_size, patience, lambd, initial_k = 1):\n",
    "    \"\"\"\n",
    "    Implements stochastic gradient descent with early stopping, adjusting the learning rate dynamically.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Input feature matrix.\n",
    "        Y (numpy.ndarray): Output target vector.\n",
    "        parameters (dict): Initial neural network parameters.\n",
    "        feed_forward (function): Function to perform the forward pass.\n",
    "        backward_propagation (function): Function to perform the backward pass.\n",
    "        act (list): List of activation functions for each layer.\n",
    "        batch_size (int): Size of each batch for gradient descent updates.\n",
    "        patience (int): Number of epochs to continue without improvement before stopping.\n",
    "        lambd (float): Regularization strength.\n",
    "        initial_k (int): Initial value for the learning rate denominator.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best parameters and the corresponding validation error.\n",
    "    \"\"\"\n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    \n",
    "    best_params = parameters.copy()\n",
    "    best_val_error = float('inf') #infinity, the highest value\n",
    "    no_improvement = 0\n",
    "    k = initial_k  # Initialize k for adaptive learning rate\n",
    "\n",
    "    n_train = len(y_train)  # Number of rows in training data\n",
    "    indices = np.arange(n_train)\n",
    "\n",
    "    while no_improvement < patience:\n",
    "        np.random.shuffle(indices)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = start + batch_size \n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X, batch_Y = X_train[batch_indices], y_train[batch_indices]\n",
    "            \n",
    "            # Perform a forward pass\n",
    "            forward = feed_forward(batch_X, len(act), act, parameters)\n",
    "            print(forward[\"H5\"].shape)\n",
    "            print(batch_X.shape)\n",
    "            # Compute gradients\n",
    "            grads = backward_propagation(loss, len(act), nh, batch_Y, lambd, parameters, forward, act)\n",
    "        \n",
    "            # Update parameters using a dynamic step size 1/k\n",
    "            step_size = 1 / k\n",
    "            for l in range(1, len(parameters) // 2 + 1):\n",
    "                parameters[\"W\" + str(l)] -= step_size * grads[\"dW\" + str(l)]\n",
    "                parameters[\"B\" + str(l)] -= step_size * grads[\"dB\" + str(l)]\n",
    "            \n",
    "        # After each epoch, evaluate on the validation set\n",
    "        val_forward = feed_forward(X_val, len(act), act, parameters)\n",
    "        val_error = np.linalg.norm(y_val - val_forward[\"H\" + str(len(act))].T)  # Using L2 norm for simplicity\n",
    "\n",
    "        # Update the best parameters if the current model is better\n",
    "        if val_error < best_val_error:\n",
    "            best_val_error = val_error\n",
    "            best_params = {k: v.copy() for k, v in parameters.items()}\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        # Increment k after each full pass through the data\n",
    "        k += 1\n",
    "\n",
    "    return {'best_params': best_params, 'best_val_error': best_val_error}\n",
    "\n",
    "stoch_grad_des_w_early_stop(\"MSE\", nh, X, Y, parameters, act, batch_size, patience, lambd, initial_k = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98290dbb",
   "metadata": {},
   "source": [
    "### Part 5a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09cd9f",
   "metadata": {},
   "source": [
    "When initializing neural networks, the choice of starting values for the weights and biases can significantly impact the performance of the model, particularly in terms of convergence speed and the likelihood of reaching a good local minimum. I am choosing random values as my initial weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "42df8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter(nl, nh, X):\n",
    "    nh.insert(0, X.shape[1]) # add the row of the X's to the first one\n",
    "    nh[-1] = X.shape[0] #changing the \"1\" to be the layer of neur\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) of a neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - nl (int): Number of layers in the neural network.\n",
    "    - nh (list): List containing the number of neurons in each layer.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing the initialized (random parameters).\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for n in range(1, nl + 1):\n",
    "        # Initialize weights randomly from a uniform distribution\n",
    "        parameters[\"W\" + str(n)] = np.random.rand(nh[n], nh[n-1])\n",
    "        # Initialize biases as zeros\n",
    "        parameters[\"B\" + str(n)] = np.zeros((nh[n], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd3d0d",
   "metadata": {},
   "source": [
    "### Part 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595edd44",
   "metadata": {},
   "source": [
    "Normalization is highly recommended, primarily because it helps in speeding up the training process and achieving better convergence. Here are the main reasons for normalizing data when training neural networks. Neural networks often perform better when input features are on a similar scale. This uniformity prevents certain features from disproportionately influencing the model's learning, which is particularly important for weights to converge more quickly during gradient descent. Moreover, when data are not normalized, the gradients during training can become very small (vanish) or very large (explode), especially in deep networks. This can lead to unstable training processes, where the network either learns too slowly or diverges. Relatedly, normalization ensures that the error surface is more spherical. Without normalization, the surface can be elongated, meaning gradient descent takes longer paths towards minima, potentially oscillating inefficiently down steep slopes.\n",
    "\n",
    "The decision on whether to penalize weights, biases, or both in a neural network generally hinges on the goals of regularization and the model:\n",
    "\n",
    "1. **Penalizing Weights**: \n",
    "   - Regularization techniques like $L^2$ (Ridge) and $L^1$ (Lasso) are typically applied to weights. Penalizing weights helps in controlling the model's complexity by keeping the weights small, which encourages the model to find simpler patterns that may generalize better on unseen data.\n",
    "   - By keeping weights small, it reduces the risk of the model fitting too closely to the noise in the training data.\n",
    "\n",
    "2. **Penalizing Biases**:\n",
    "   - It is generally less common to penalize biases because they do not control the complexity of the model in the same way that weights do. Biases are meant to provide flexibility to the model by allowing shifts to the decision boundary.\n",
    "   - Penalizing biases can lead to underfitting since it restricts the model's ability to fit even the correct general trend of the data.\n",
    "\n",
    "It's usually best to penalize just the weights because this directly addresses the model's complexity without unduly limiting its capacity to fit the data appropriately. Since biases help in fitting the model more flexibly to the data without significantly increasing the complexity, they are typically left unpenalized to maintain the model's adaptive performance across varying data dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae2607",
   "metadata": {},
   "source": [
    "### Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 5\n",
    "lambd = [1, 10, 100, 1000, 10000]\n",
    "loss = [\"MSE\", \"CE\"]\n",
    "\n",
    "for lam in lambd:\n",
    "    for los in loss:\n",
    "        stoch_grad_des_w_early_stop(los, nh, X, Y, parameters, \n",
    "                                    feed_forward, backward_propagation, \n",
    "                                    act, batch_size, patience, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2489a2c",
   "metadata": {},
   "source": [
    "### Part 7(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b035b6f",
   "metadata": {},
   "source": [
    "Adding 50% more data to an existing dataset and retraining the model inherently changes the dynamics of how the model interacts with the data. If you randomly split the combined dataset into training, validation, and test sets, the key to ensuring that the test error estimate is unbiased relies on the independence of the test set. If the test data contains examples that are not representative of the overall dataset or are similar to the training set, the test error might not accurately reflect how the model will perform on truly unseen data. Moreover, there's a risk of data leakage if the test set includes data that's too similar to or influenced by the training data. This could happen if the new data shares specific characteristics or patterns with the old data that the model has already learned.\n",
    "\n",
    "To ensure the test error estimate remains unbiased, we could consider splitting the data based on time (if it's time-series data) or some logical separation that ensures the test set is likely to encounter scenarios the model hasn't explicitly trained on. This helps mimic real-world application where future data or different scenarios are encountered. If the dataset has classes or categories, use stratified sampling to ensure that the training, validation, and test sets each contain a representative mix of all classes. This prevents class imbalance in any of the sets, which could bias the model's performance and thus the error estimate. From the combined dataset, allocate a portion as a hold-out test set before training begins. This set should not be used in any way during the model training or tuning processes, including not influencing the selection of $ \\lambda $. It serves as a final evaluator to test the model's performance. Finally, we would want to consider using k-fold cross-validation, especially if data is limited, to ensure that every data point gets used for both training and testing. This helps in understanding the model's stability and robustness across different subsets of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706f433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
